{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_header"
   },
   "source": [
    "# Tako HRM - Setup & Performance Benchmark\n",
    "\n",
    "This notebook sets up the Tako environment on Google Colab and runs performance benchmarks.\n",
    "\n",
    "## üöÄ Quick Start\n",
    "\n",
    "1. **Enable GPU:** Runtime ‚Üí Change runtime type ‚Üí GPU (T4 or better)\n",
    "2. **Run all cells:** Runtime ‚Üí Run all\n",
    "3. **Check benchmark results** at the bottom\n",
    "\n",
    "---\n",
    "\n",
    "## Setup Steps\n",
    "\n",
    "- Clone repository\n",
    "- Install dependencies with `uv`\n",
    "- Verify GPU availability\n",
    "- Run performance benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "check_gpu"
   },
   "source": [
    "## 1. Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpu_check"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"GPU Check\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"‚úÖ CUDA GPU Available: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    print(\"‚úÖ Apple MPS Available (Metal Performance Shaders)\")\n",
    "    device = 'mps'\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No GPU available, using CPU (will be slower)\")\n",
    "    device = 'cpu'\n",
    "\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "clone_repo"
   },
   "source": [
    "## 2. Clone Repository & Install Dependencies\n",
    "\n",
    "**Note:** If you already have the repo, skip the clone and just `cd` into it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_uv"
   },
   "outputs": [],
   "source": [
    "# Install uv (fast Python package manager)\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Add uv to PATH for this session\n",
    "import os\n",
    "os.environ['PATH'] = f\"{os.path.expanduser('~/.cargo/bin')}:{os.environ['PATH']}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_and_setup"
   },
   "outputs": [],
   "source": [
    "# Clone repository (modify URL to your fork if needed)\n",
    "import os\n",
    "if not os.path.exists('tako-v2'):\n",
    "    !git clone https://github.com/zfdupont/tako-v2.git\n",
    "    print(\"‚úÖ Repository cloned\")\n",
    "else:\n",
    "    print(\"‚úÖ Repository already exists\")\n",
    "\n",
    "%cd tako-v2\n",
    "\n",
    "# Install dependencies\n",
    "!~/.cargo/bin/uv sync\n",
    "\n",
    "print(\"\\n‚úÖ Dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mount_drive"
   },
   "source": [
    "## 3. Mount Google Drive (Optional)\n",
    "\n",
    "Mount your Google Drive to save checkpoints and results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_gdrive"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create checkpoint directory in Drive\n",
    "!mkdir -p /content/drive/MyDrive/tako_checkpoints/tictactoe\n",
    "\n",
    "# Link to local checkpoint directory\n",
    "!rm -rf checkpoints\n",
    "!ln -s /content/drive/MyDrive/tako_checkpoints checkpoints\n",
    "\n",
    "print(\"‚úÖ Google Drive mounted and checkpoints linked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "verify_install"
   },
   "source": [
    "## 4. Verify Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "test_imports"
   },
   "outputs": [],
   "source": [
    "# Test imports\n",
    "import sys\n",
    "sys.path.insert(0, '/content/tako-v2')\n",
    "\n",
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from model.hrm import HRM\n",
    "from games.tictactoe import TicTacToeGame\n",
    "from training.mcts import MCTS\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"   PyTorch version: {torch.__version__}\")\n",
    "print(f\"   CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "benchmark_header"
   },
   "source": [
    "## 5. Run Performance Benchmark\n",
    "\n",
    "Measure forward pass time and estimate training throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "benchmark_code"
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import time\n",
    "from model.hrm import HRM\n",
    "from games.tictactoe import TicTacToeGame\n",
    "\n",
    "# Determine device\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TicTacToe Performance Benchmark\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDevice: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "# Load config\n",
    "with open('config/tictactoe.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "model_config = config['model']\n",
    "mcts_config = config['mcts']\n",
    "\n",
    "print(f\"\\nModel Configuration:\")\n",
    "print(f\"  d_model: {model_config['d_model']}\")\n",
    "print(f\"  n_layers: {model_config['n_layers']}\")\n",
    "print(f\"  N√óT: {model_config['N']}√ó{model_config['T']} = {model_config['N']*model_config['T']} timesteps/segment\")\n",
    "print(f\"\\nMCTS Configuration:\")\n",
    "print(f\"  Simulations: {mcts_config['simulations']}\")\n",
    "print(f\"  max_segments_inference: {mcts_config.get('max_segments_inference', 1)}\")\n",
    "\n",
    "# Create model\n",
    "model = HRM(**model_config)\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nModel Parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "\n",
    "# Create dummy input\n",
    "game = TicTacToeGame()\n",
    "tokens = game.to_tokens().unsqueeze(0).to(device)  # [1, seq_len]\n",
    "\n",
    "# Warmup\n",
    "print(f\"\\nWarming up...\")\n",
    "with torch.no_grad():\n",
    "    for _ in range(5):\n",
    "        _ = model.predict(tokens, use_act=True, max_segments=mcts_config.get('max_segments_inference', 1))\n",
    "\n",
    "# Benchmark\n",
    "print(f\"Benchmarking (20 iterations)...\")\n",
    "times = []\n",
    "with torch.no_grad():\n",
    "    for _ in range(20):\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        start = time.time()\n",
    "        policy, value, _ = model.predict(\n",
    "            tokens, use_act=True, \n",
    "            max_segments=mcts_config.get('max_segments_inference', 1)\n",
    "        )\n",
    "        \n",
    "        if device == 'cuda':\n",
    "            torch.cuda.synchronize()\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        times.append(elapsed)\n",
    "\n",
    "# Results\n",
    "avg_time = sum(times) / len(times)\n",
    "min_time = min(times)\n",
    "max_time = max(times)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Forward Pass Results\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Average: {avg_time*1000:.2f}ms\")\n",
    "print(f\"  Min: {min_time*1000:.2f}ms\")\n",
    "print(f\"  Max: {max_time*1000:.2f}ms\")\n",
    "\n",
    "# Estimate game generation time\n",
    "avg_moves = 7\n",
    "sims = mcts_config['simulations']\n",
    "total_passes = sims * avg_moves\n",
    "est_time_per_game = avg_time * total_passes\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"Estimated Training Throughput\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  MCTS simulations: {sims}\")\n",
    "print(f\"  Avg moves per game: {avg_moves}\")\n",
    "print(f\"  Forward passes per game: {total_passes}\")\n",
    "print(f\"  Time per game: {est_time_per_game:.2f}s\")\n",
    "print(f\"  Games per hour (1 worker): {3600/est_time_per_game:.0f}\")\n",
    "print(f\"  Games per hour (8 workers): {8*3600/est_time_per_game:.0f}\")\n",
    "\n",
    "# Speedup vs CPU baseline\n",
    "cpu_baseline = 0.311  # 311ms from CPU benchmark\n",
    "baseline_time_per_game = cpu_baseline * 200 * avg_moves  # 200 sims\n",
    "speedup = baseline_time_per_game / est_time_per_game\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Speedup vs CPU Baseline (311ms/pass, 200 sims)\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"  Baseline: {baseline_time_per_game:.1f}s per game\")\n",
    "print(f\"  Current: {est_time_per_game:.2f}s per game\")\n",
    "print(f\"  Speedup: {speedup:.1f}x faster\")\n",
    "print(f\"\\n‚úÖ Benchmark complete!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## üéØ Next Steps\n",
    "\n",
    "Now that setup is complete, you can:\n",
    "\n",
    "1. **Train a model:** Open `01_train_tictactoe.ipynb`\n",
    "2. **Evaluate a model:** Open `02_evaluate_model.ipynb`\n",
    "3. **Play interactively:** Open `03_interactive_play.ipynb`\n",
    "\n",
    "---\n",
    "\n",
    "### Expected Performance on Colab GPUs\n",
    "\n",
    "| GPU Type | Forward Pass | Games/Hour (8 workers) |\n",
    "|----------|--------------|------------------------|\n",
    "| **T4** | ~1-2ms | ~150,000 |\n",
    "| **V100** | ~0.5-1ms | ~300,000 |\n",
    "| **A100** | ~0.3-0.5ms | ~500,000 |\n",
    "| **CPU** | ~3-5ms | ~50,000 |\n",
    "\n",
    "*(Actual results may vary based on system load)*"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
   }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
