{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_header"
   },
   "source": [
    "# Tako HRM - TicTacToe Training\n",
    "\n",
    "Train the Hierarchical Reasoning Model (HRM) on TicTacToe using self-play reinforcement learning.\n",
    "\n",
    "## üöÄ Quick Start\n",
    "\n",
    "1. **Enable GPU:** Runtime ‚Üí Change runtime type ‚Üí GPU (T4 or better)\n",
    "2. **Run setup cells** (sections 1-2)\n",
    "3. **Start training** (section 3)\n",
    "4. **Monitor progress** with live visualizations\n",
    "\n",
    "---\n",
    "\n",
    "## Training Pipeline\n",
    "\n",
    "- **Self-play workers:** Generate games using MCTS + current model\n",
    "- **Replay buffer:** Store positions from recent games (~500K positions)\n",
    "- **Learner:** Train model on sampled batches (policy + value + ACT loss)\n",
    "- **Checkpointing:** Save model every 500 steps\n",
    "- **Evaluation:** Test against random play periodically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_section"
   },
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_imports"
   },
   "outputs": [],
   "source": [
    "# Install uv if needed\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Add to PATH\n",
    "import os\n",
    "os.environ['PATH'] = f\"{os.path.expanduser('~/.cargo/bin')}:{os.environ['PATH']}\"\n",
    "\n",
    "# Clone repo if needed\n",
    "if not os.path.exists('tako-v2'):\n",
    "    !git clone https://github.com/zfdupont/tako-v2.git\n",
    "    %cd tako-v2\n",
    "    !~/.cargo/bin/uv sync\n",
    "else:\n",
    "    %cd tako-v2\n",
    "\n",
    "print(\"‚úÖ Environment ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive for checkpoint persistence\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Link checkpoint directory\n",
    "!mkdir -p /content/drive/MyDrive/tako_checkpoints/tictactoe\n",
    "!rm -rf checkpoints\n",
    "!ln -s /content/drive/MyDrive/tako_checkpoints checkpoints\n",
    "\n",
    "print(\"‚úÖ Checkpoints will be saved to Google Drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "check_gpu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"‚ö†Ô∏è No GPU detected - training will be slower\")\n",
    "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config_section"
   },
   "source": [
    "## 2. Training Configuration\n",
    "\n",
    "View and optionally modify the training config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "load_config"
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "from pprint import pprint\n",
    "\n",
    "# Load config\n",
    "with open('config/tictactoe.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "print(\"Current TicTacToe Configuration:\")\n",
    "print(\"=\"*80)\n",
    "pprint(config)\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "optional_config_override"
   },
   "outputs": [],
   "source": [
    "# OPTIONAL: Override config parameters for faster experimentation\n",
    "# Uncomment and modify as needed\n",
    "\n",
    "# config['selfplay']['num_workers'] = 4  # Reduce workers if OOM\n",
    "# config['training']['batch_size'] = 256  # Reduce batch size if OOM\n",
    "# config['checkpointing']['save_interval'] = 100  # Save more frequently\n",
    "\n",
    "# Save modified config\n",
    "# with open('config/tictactoe_colab.yaml', 'w') as f:\n",
    "#     yaml.dump(config, f)\n",
    "# print(\"‚úÖ Config overrides saved to config/tictactoe_colab.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_section"
   },
   "source": [
    "## 3. Start Training\n",
    "\n",
    "Launch the training process with Ray distributed workers.\n",
    "\n",
    "**Note:** Training runs in a separate process. Check the output logs for progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_command"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "NUM_EPOCHS = 5  # Number of epochs to train\n",
    "CONFIG_FILE = 'config/tictactoe.yaml'  # Use tictactoe_colab.yaml if you modified config\n",
    "\n",
    "print(f\"Starting training for {NUM_EPOCHS} epochs...\")\n",
    "print(f\"Config: {CONFIG_FILE}\")\n",
    "print(f\"Device: {device}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Training logs will appear below. Press Ctrl+C to stop.\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Run training\n",
    "!~/.cargo/bin/uv run python scripts/train.py --config {CONFIG_FILE} --epochs {NUM_EPOCHS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "monitor_section"
   },
   "source": [
    "## 4. Monitor Training Progress\n",
    "\n",
    "Load and visualize training metrics from logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "parse_logs"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Find latest log file\n",
    "log_dir = Path('logs')\n",
    "if log_dir.exists():\n",
    "    log_files = list(log_dir.glob('*.log'))\n",
    "    if log_files:\n",
    "        latest_log = max(log_files, key=lambda p: p.stat().st_mtime)\n",
    "        print(f\"Reading log: {latest_log}\")\n",
    "        \n",
    "        # Parse training metrics\n",
    "        steps = []\n",
    "        losses = []\n",
    "        policy_losses = []\n",
    "        value_losses = []\n",
    "        \n",
    "        with open(latest_log) as f:\n",
    "            for line in f:\n",
    "                # Match learner log lines with loss values\n",
    "                if 'loss=' in line:\n",
    "                    # Extract step and losses\n",
    "                    step_match = re.search(r'step=(\\d+)', line)\n",
    "                    loss_match = re.search(r'loss=([\\d.]+)', line)\n",
    "                    policy_match = re.search(r'policy=([\\d.]+)', line)\n",
    "                    value_match = re.search(r'value=([\\d.]+)', line)\n",
    "                    \n",
    "                    if step_match and loss_match:\n",
    "                        steps.append(int(step_match.group(1)))\n",
    "                        losses.append(float(loss_match.group(1)))\n",
    "                        if policy_match:\n",
    "                            policy_losses.append(float(policy_match.group(1)))\n",
    "                        if value_match:\n",
    "                            value_losses.append(float(value_match.group(1)))\n",
    "        \n",
    "        if steps:\n",
    "            # Plot training curves\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "            \n",
    "            # Total loss\n",
    "            axes[0].plot(steps, losses, alpha=0.6, label='Total Loss')\n",
    "            axes[0].set_xlabel('Training Step')\n",
    "            axes[0].set_ylabel('Loss')\n",
    "            axes[0].set_title('Total Loss')\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Policy loss\n",
    "            if policy_losses:\n",
    "                axes[1].plot(steps[:len(policy_losses)], policy_losses, alpha=0.6, color='orange', label='Policy Loss')\n",
    "                axes[1].set_xlabel('Training Step')\n",
    "                axes[1].set_ylabel('Loss')\n",
    "                axes[1].set_title('Policy Loss')\n",
    "                axes[1].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Value loss\n",
    "            if value_losses:\n",
    "                axes[2].plot(steps[:len(value_losses)], value_losses, alpha=0.6, color='green', label='Value Loss')\n",
    "                axes[2].set_xlabel('Training Step')\n",
    "                axes[2].set_ylabel('Loss')\n",
    "                axes[2].set_title('Value Loss')\n",
    "                axes[2].grid(True, alpha=0.3)\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"\\n‚úÖ Parsed {len(steps)} training steps\")\n",
    "            if losses:\n",
    "                print(f\"   Latest loss: {losses[-1]:.4f}\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No training metrics found in log\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No log files found\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Logs directory not found. Run training first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "checkpoint_section"
   },
   "source": [
    "## 5. List Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "list_checkpoints"
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import datetime\n",
    "\n",
    "checkpoint_dir = Path('checkpoints/tictactoe')\n",
    "\n",
    "if checkpoint_dir.exists():\n",
    "    checkpoints = sorted(checkpoint_dir.glob('*.pt'), key=lambda p: p.stat().st_mtime)\n",
    "    \n",
    "    if checkpoints:\n",
    "        print(f\"Found {len(checkpoints)} checkpoint(s):\\n\")\n",
    "        print(f\"{'Name':<40} {'Size':<10} {'Modified'}\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for ckpt in checkpoints:\n",
    "            size_mb = ckpt.stat().st_size / 1e6\n",
    "            mtime = datetime.datetime.fromtimestamp(ckpt.stat().st_mtime)\n",
    "            print(f\"{ckpt.name:<40} {size_mb:>6.1f} MB   {mtime.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        \n",
    "        print(\"\\n‚úÖ Latest checkpoint:\", checkpoints[-1].name)\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No checkpoints found. Train first.\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Checkpoint directory not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "next_steps"
   },
   "source": [
    "## üéØ Next Steps\n",
    "\n",
    "After training:\n",
    "\n",
    "1. **Evaluate model:** Open `02_evaluate_model.ipynb` to test against random/perfect play\n",
    "2. **Play interactively:** Open `03_interactive_play.ipynb` to play against your trained model\n",
    "3. **Continue training:** Re-run section 3 to train for more epochs\n",
    "\n",
    "---\n",
    "\n",
    "### Training Tips\n",
    "\n",
    "- **GPU memory issues?** Reduce `num_workers` or `batch_size` in config\n",
    "- **Slow convergence?** TicTacToe is simple - should converge in ~1000 games\n",
    "- **Check win rate:** Should reach >90% vs random play within 30 minutes\n",
    "- **Checkpoints persist** in Google Drive even if session disconnects"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
   }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
