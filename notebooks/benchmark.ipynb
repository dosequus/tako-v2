{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tako HRM - MCTS Benchmark\n",
    "\n",
    "Benchmark MCTS search performance across different configurations and games.\n",
    "\n",
    "## What This Measures\n",
    "\n",
    "- **MCTS searches/second** - How fast can we search game trees?\n",
    "- **Forward pass time** - Neural network inference speed\n",
    "- **Batching efficiency** - Speedup from batched evaluation\n",
    "- **GPU vs CPU** - Device comparison\n",
    "- **Game complexity** - How game size affects performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Setup\n",
    "\n",
    "**Run `setup.ipynb` first if you haven't already!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CUDA GPU: Tesla T4\n",
      "   Memory: 15.6 GB\n",
      "‚úÖ CPU\n",
      "\n",
      "‚úÖ Setup verified - ready to benchmark!\n",
      "Devices to test: ['cuda', 'cpu']\n"
     ]
    }
   ],
   "source": [
    "# Verify setup and import libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import yaml\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.chdir('/content/tako-v2')\n",
    "\n",
    "if not os.path.exists('scripts/train.py'):\n",
    "    print(\"‚ùå ERROR: Not in tako-v2 directory\")\n",
    "    print(\"   Run setup.ipynb first!\")\n",
    "    raise FileNotFoundError(\"Run setup.ipynb first\")\n",
    "\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "from model.hrm import HRM\n",
    "from training.mcts import MCTS\n",
    "from games.tictactoe import TicTacToeGame\n",
    "from games.othello import OthelloGame\n",
    "\n",
    "# Detect available devices\n",
    "devices = []\n",
    "if torch.cuda.is_available():\n",
    "    devices.append('cuda')\n",
    "    print(f\"‚úÖ CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "if torch.backends.mps.is_available():\n",
    "    devices.append('mps')\n",
    "    print(f\"‚úÖ Apple MPS\")\n",
    "devices.append('cpu')\n",
    "print(f\"‚úÖ CPU\")\n",
    "\n",
    "print(f\"\\n‚úÖ Setup verified - ready to benchmark!\")\n",
    "print(f\"Devices to test: {devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CUDA GPU: Tesla T4\n",
      "   Memory: 15.6 GB\n",
      "‚úÖ CPU\n",
      "\n",
      "Devices to benchmark: ['cuda', 'cpu']\n"
     ]
    }
   ],
   "source": [
    "# Check available devices\n",
    "import torch\n",
    "\n",
    "devices = []\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    devices.append('cuda')\n",
    "    print(f\"‚úÖ CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    devices.append('mps')\n",
    "    print(f\"‚úÖ Apple MPS (Metal Performance Shaders)\")\n",
    "\n",
    "devices.append('cpu')\n",
    "print(f\"‚úÖ CPU\")\n",
    "\n",
    "print(f\"\\nDevices to benchmark: {devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Forward pass benchmark function ready\n"
     ]
    }
   ],
   "source": [
    "def benchmark_forward_pass(game_name, device, num_trials=100, use_optimizations=True):\n",
    "    \"\"\"Benchmark forward pass speed with optional optimizations.\n",
    "    \n",
    "    Args:\n",
    "        game_name: Game to benchmark ('tictactoe', 'othello')\n",
    "        device: Device to use ('cuda', 'cpu', 'mps')\n",
    "        num_trials: Number of trials to run\n",
    "        use_optimizations: Enable torch.compile + bfloat16 + inference_mode (default: True)\n",
    "    \"\"\"\n",
    "    # Load config\n",
    "    with open(f'config/{game_name}.yaml') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Create model\n",
    "    model = HRM(**config['model'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Apply optimizations if requested\n",
    "    if use_optimizations:\n",
    "        if device == 'cuda':\n",
    "            dtype = torch.bfloat16\n",
    "            use_compile = True\n",
    "        else:\n",
    "            dtype = None  # Keep float32 on CPU/MPS\n",
    "            use_compile = False  # torch.compile less beneficial on CPU\n",
    "        \n",
    "        if dtype is not None or use_compile:\n",
    "            model.optimize_for_inference(use_compile=use_compile, dtype=dtype)\n",
    "    \n",
    "    # Create game for tokens\n",
    "    if game_name == 'tictactoe':\n",
    "        game = TicTacToeGame()\n",
    "    elif game_name == 'othello':\n",
    "        game = OthelloGame()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown game: {game_name}\")\n",
    "    \n",
    "    tokens = game.to_tokens().unsqueeze(0).to(device)\n",
    "    max_segments = config['mcts'].get('max_segments_inference', 1)\n",
    "    \n",
    "    # Warmup (extra warmup for torch.compile first-run compilation)\n",
    "    warmup_iterations = 20 if use_optimizations else 10\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup_iterations):\n",
    "            policy, value, _ = model.predict(tokens, use_act=True, max_segments=max_segments)\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm(range(num_trials), desc=f\"{game_name} on {device}\"):\n",
    "            start = time.time()\n",
    "            policy, value, _ = model.predict(tokens, use_act=True, max_segments=max_segments)\n",
    "            if device == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            times.append(time.time() - start)\n",
    "    \n",
    "    avg_time = np.mean(times) * 1000  # Convert to ms\n",
    "    std_time = np.std(times) * 1000\n",
    "    \n",
    "    return {\n",
    "        'game': game_name,\n",
    "        'device': device,\n",
    "        'avg_ms': avg_time,\n",
    "        'std_ms': std_time,\n",
    "        'params': sum(p.numel() for p in model.parameters()) / 1e6,\n",
    "        'max_segments': max_segments,\n",
    "        'optimized': use_optimizations\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Forward pass benchmark function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚ö†Ô∏è  tictactoe on cuda failed: 'HRM' object has no attribute 'optimize_for_inference'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tictactoe on cpu: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:00<00:00, 141.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tictactoe on cpu: 7.01 ¬± 1.05 ms\n",
      "\n",
      "‚ö†Ô∏è  othello on cuda failed: 'HRM' object has no attribute 'optimize_for_inference'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "othello on cpu:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 47/100 [00:47<01:00,  1.14s/it]"
     ]
    }
   ],
   "source": [
    "# Run forward pass benchmarks\n",
    "forward_results = []\n",
    "\n",
    "games_to_test = ['tictactoe', 'othello']\n",
    "\n",
    "for game in games_to_test:\n",
    "    for device in devices:\n",
    "        try:\n",
    "            result = benchmark_forward_pass(game, device, num_trials=100)\n",
    "            forward_results.append(result)\n",
    "            print(f\"\\n{game} on {device}: {result['avg_ms']:.2f} ¬± {result['std_ms']:.2f} ms\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è  {game} on {device} failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Forward Pass Benchmark Results:\")\n",
    "print(\"=\"*80)\n",
    "for r in forward_results:\n",
    "    print(f\"{r['game']:<12} {r['device']:<6} {r['avg_ms']:>7.2f} ms  ({r['params']:.1f}M params, {r['max_segments']} seg)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Benchmark 2: MCTS Search Speed\n",
    "\n",
    "Measure complete MCTS search including tree traversal + neural evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_mcts_search(game_name, device, num_searches=50, simulations=25):\n",
    "    \"\"\"Benchmark MCTS search speed.\"\"\"\n",
    "    # Load config\n",
    "    with open(f'config/{game_name}.yaml') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Override simulations\n",
    "    config['mcts']['simulations'] = simulations\n",
    "    \n",
    "    # Create model\n",
    "    model = HRM(**config['model'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create game class\n",
    "    if game_name == 'tictactoe':\n",
    "        game_class = TicTacToeGame\n",
    "    elif game_name == 'othello':\n",
    "        game_class = OthelloGame\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown game: {game_name}\")\n",
    "    \n",
    "    # Create MCTS\n",
    "    mcts = MCTS(model, game_class, config['mcts'], device=device)\n",
    "    \n",
    "    # Warmup\n",
    "    game = game_class()\n",
    "    for _ in range(5):\n",
    "        _ = mcts.search(game, move_num=0)\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    for _ in tqdm(range(num_searches), desc=f\"{game_name} MCTS on {device}\"):\n",
    "        game = game_class()\n",
    "        start = time.time()\n",
    "        policy = mcts.search(game, move_num=0)  # Returns policy distribution\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    \n",
    "    return {\n",
    "        'game': game_name,\n",
    "        'device': device,\n",
    "        'simulations': simulations,\n",
    "        'avg_sec': avg_time,\n",
    "        'std_sec': std_time,\n",
    "        'searches_per_sec': 1.0 / avg_time,\n",
    "        'batch_size': config['mcts'].get('batch_size', 1)\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ MCTS benchmark function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MCTS benchmarks with default config\n",
    "mcts_results = []\n",
    "\n",
    "games_to_test = ['tictactoe', 'othello']\n",
    "\n",
    "for game in games_to_test:\n",
    "    for device in devices:\n",
    "        try:\n",
    "            result = benchmark_mcts_search(game, device, num_searches=50, simulations=25)\n",
    "            mcts_results.append(result)\n",
    "            print(f\"\\n{game} on {device}:\")\n",
    "            print(f\"  {result['avg_sec']:.3f} ¬± {result['std_sec']:.3f} sec/search\")\n",
    "            print(f\"  {result['searches_per_sec']:.1f} searches/sec\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è  {game} on {device} failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MCTS Search Benchmark Results (25 simulations):\")\n",
    "print(\"=\"*80)\n",
    "for r in mcts_results:\n",
    "    print(f\"{r['game']:<12} {r['device']:<6} {r['avg_sec']:>6.3f} s  ({r['searches_per_sec']:>6.1f} searches/s, batch={r['batch_size']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Benchmark 3: Scaling with Simulations\n",
    "\n",
    "How does MCTS performance scale with different numbers of simulations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark MCTS with different simulation counts\n",
    "GAME = \"tictactoe\"  # Change to 'othello' for larger game\n",
    "DEVICE = devices[0]  # Use best available device\n",
    "\n",
    "simulation_counts = [10, 25, 50, 100, 200, 400]\n",
    "scaling_results = []\n",
    "\n",
    "print(f\"Benchmarking {GAME} on {DEVICE} with varying simulations...\\n\")\n",
    "\n",
    "for sims in simulation_counts:\n",
    "    result = benchmark_mcts_search(GAME, DEVICE, num_searches=30, simulations=sims)\n",
    "    scaling_results.append(result)\n",
    "    print(f\"{sims:>4} sims: {result['avg_sec']:.3f} s ({result['searches_per_sec']:.1f} searches/s)\")\n",
    "\n",
    "print(\"\\n‚úÖ Scaling benchmark complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scaling results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "sims = [r['simulations'] for r in scaling_results]\n",
    "times = [r['avg_sec'] for r in scaling_results]\n",
    "throughput = [r['searches_per_sec'] for r in scaling_results]\n",
    "\n",
    "# Time vs simulations\n",
    "axes[0].plot(sims, times, 'o-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('MCTS Simulations', fontsize=12)\n",
    "axes[0].set_ylabel('Time per Search (seconds)', fontsize=12)\n",
    "axes[0].set_title(f'{GAME.capitalize()} - Search Time vs Simulations', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xscale('log')\n",
    "\n",
    "# Throughput vs simulations\n",
    "axes[1].plot(sims, throughput, 'o-', linewidth=2, markersize=8, color='green')\n",
    "axes[1].set_xlabel('MCTS Simulations', fontsize=12)\n",
    "axes[1].set_ylabel('Searches per Second', fontsize=12)\n",
    "axes[1].set_title(f'{GAME.capitalize()} - Throughput vs Simulations', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Observations:\")\n",
    "print(f\"  - Time scales approximately linearly with simulations\")\n",
    "print(f\"  - Throughput (searches/sec) decreases with more simulations\")\n",
    "print(f\"  - For training, balance: more sims = stronger play, fewer sims = faster iteration\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Benchmark 4: Batching Efficiency\n",
    "\n",
    "Compare batched vs non-batched MCTS evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_batching(game_name, device, batch_sizes=[1, 4, 8, 16, 32]):\n",
    "    \"\"\"Benchmark different batch sizes.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        # Load config\n",
    "        with open(f'config/{game_name}.yaml') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        \n",
    "        # Override batch size\n",
    "        config['mcts']['batch_size'] = batch_size\n",
    "        config['mcts']['simulations'] = 100  # Fixed for comparison\n",
    "        \n",
    "        # Create model\n",
    "        model = HRM(**config['model'])\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        # Create game class\n",
    "        if game_name == 'tictactoe':\n",
    "            game_class = TicTacToeGame\n",
    "        elif game_name == 'othello':\n",
    "            game_class = OthelloGame\n",
    "        \n",
    "        # Create MCTS\n",
    "        mcts = MCTS(model, game_class, config['mcts'], device=device)\n",
    "        \n",
    "        # Warmup\n",
    "        game = game_class()\n",
    "        for _ in range(3):\n",
    "            _ = mcts.search(game, move_num=0)\n",
    "        \n",
    "        # Benchmark\n",
    "        times = []\n",
    "        for _ in range(20):\n",
    "            game = game_class()\n",
    "            start = time.time()\n",
    "            policy = mcts.search(game, move_num=0)  # Returns policy distribution\n",
    "            times.append(time.time() - start)\n",
    "        \n",
    "        avg_time = np.mean(times)\n",
    "        results.append({\n",
    "            'batch_size': batch_size,\n",
    "            'avg_sec': avg_time,\n",
    "            'searches_per_sec': 1.0 / avg_time\n",
    "        })\n",
    "        \n",
    "        print(f\"Batch size {batch_size:>2}: {avg_time:.3f} s ({1.0/avg_time:.1f} searches/s)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Batching benchmark function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run batching benchmark\n",
    "GAME = \"tictactoe\"\n",
    "DEVICE = devices[0]\n",
    "\n",
    "print(f\"Benchmarking batching for {GAME} on {DEVICE}...\\n\")\n",
    "batching_results = benchmark_batching(GAME, DEVICE, batch_sizes=[1, 4, 8, 16, 32])\n",
    "\n",
    "print(\"\\n‚úÖ Batching benchmark complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot batching efficiency\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "batch_sizes = [r['batch_size'] for r in batching_results]\n",
    "times = [r['avg_sec'] for r in batching_results]\n",
    "throughput = [r['searches_per_sec'] for r in batching_results]\n",
    "\n",
    "# Speedup vs batch size\n",
    "baseline_time = batching_results[0]['avg_sec']  # batch_size=1\n",
    "speedups = [baseline_time / t for t in times]\n",
    "\n",
    "axes[0].plot(batch_sizes, speedups, 'o-', linewidth=2, markersize=8)\n",
    "axes[0].axhline(y=1, color='r', linestyle='--', alpha=0.5, label='No speedup')\n",
    "axes[0].set_xlabel('Batch Size', fontsize=12)\n",
    "axes[0].set_ylabel('Speedup vs Batch=1', fontsize=12)\n",
    "axes[0].set_title(f'{GAME.capitalize()} - Batching Speedup', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "axes[0].set_xscale('log', base=2)\n",
    "\n",
    "# Efficiency (speedup / batch_size)\n",
    "efficiency = [speedup / bs for speedup, bs in zip(speedups, batch_sizes)]\n",
    "axes[1].plot(batch_sizes, efficiency, 'o-', linewidth=2, markersize=8, color='orange')\n",
    "axes[1].axhline(y=1, color='r', linestyle='--', alpha=0.5, label='Perfect scaling')\n",
    "axes[1].set_xlabel('Batch Size', fontsize=12)\n",
    "axes[1].set_ylabel('Batching Efficiency', fontsize=12)\n",
    "axes[1].set_title(f'{GAME.capitalize()} - Batching Efficiency', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "axes[1].set_xscale('log', base=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Observations:\")\n",
    "print(f\"  - Best batch size: {batch_sizes[speedups.index(max(speedups))]} ({max(speedups):.1f}x speedup)\")\n",
    "print(f\"  - GPU batching is most effective with larger batch sizes\")\n",
    "print(f\"  - CPU benefits less from batching (overhead dominates)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Benchmark 5: End-to-End Game Generation\n",
    "\n",
    "Measure complete game generation time (full playthrough)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_game_generation(game_name, device, num_games=20):\n",
    "    \"\"\"Benchmark full game generation.\"\"\"\n",
    "    # Load config\n",
    "    with open(f'config/{game_name}.yaml') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Create model\n",
    "    model = HRM(**config['model'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create game class\n",
    "    if game_name == 'tictactoe':\n",
    "        game_class = TicTacToeGame\n",
    "    elif game_name == 'othello':\n",
    "        game_class = OthelloGame\n",
    "    \n",
    "    # Create MCTS\n",
    "    mcts = MCTS(model, game_class, config['mcts'], device=device)\n",
    "    \n",
    "    # Generate games\n",
    "    times = []\n",
    "    move_counts = []\n",
    "    \n",
    "    for _ in tqdm(range(num_games), desc=f\"Generating {game_name} games\"):\n",
    "        game = game_class()\n",
    "        move_num = 0\n",
    "        \n",
    "        start = time.time()\n",
    "        while not game.is_terminal():\n",
    "            policy = mcts.search(game, move_num=move_num)  # Returns policy distribution\n",
    "            # Select move from policy\n",
    "            legal_moves = game.legal_moves()\n",
    "            legal_policy = policy[legal_moves]\n",
    "            move = legal_moves[np.argmax(legal_policy)]\n",
    "            game.make_move(move)\n",
    "            move_num += 1\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        times.append(elapsed)\n",
    "        move_counts.append(move_num)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    avg_moves = np.mean(move_counts)\n",
    "    games_per_hour = 3600 / avg_time\n",
    "    \n",
    "    return {\n",
    "        'game': game_name,\n",
    "        'device': device,\n",
    "        'avg_time': avg_time,\n",
    "        'avg_moves': avg_moves,\n",
    "        'games_per_hour': games_per_hour,\n",
    "        'simulations': config['mcts']['simulations']\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Game generation benchmark function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run game generation benchmarks\n",
    "game_gen_results = []\n",
    "\n",
    "games_to_test = ['tictactoe', 'othello']\n",
    "\n",
    "for game in games_to_test:\n",
    "    for device in devices:\n",
    "        try:\n",
    "            result = benchmark_game_generation(game, device, num_games=20)\n",
    "            game_gen_results.append(result)\n",
    "            print(f\"\\n{game} on {device}:\")\n",
    "            print(f\"  {result['avg_time']:.2f} sec/game ({result['avg_moves']:.1f} moves avg)\")\n",
    "            print(f\"  {result['games_per_hour']:.0f} games/hour\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è  {game} on {device} failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Game Generation Benchmark Results:\")\n",
    "print(\"=\"*80)\n",
    "for r in game_gen_results:\n",
    "    print(f\"{r['game']:<12} {r['device']:<6} {r['avg_time']:>6.2f} s/game  ({r['games_per_hour']:>7.0f} games/hr, {r['simulations']} sims)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary Report\n",
    "\n",
    "Comprehensive comparison across all benchmarks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summary report\n",
    "print(\"=\"*80)\n",
    "print(\"TAKO HRM - MCTS BENCHMARK SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. FORWARD PASS PERFORMANCE\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Game':<12} {'Device':<8} {'Latency (ms)':<15} {'Params':<12} {'Segments'}\")\n",
    "print(\"-\" * 80)\n",
    "for r in forward_results:\n",
    "    print(f\"{r['game']:<12} {r['device']:<8} {r['avg_ms']:>7.2f} ¬± {r['std_ms']:<5.2f} {r['params']:>7.1f}M     {r['max_segments']:>2}\")\n",
    "\n",
    "print(\"\\n2. MCTS SEARCH PERFORMANCE\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Game':<12} {'Device':<8} {'Time/Search (s)':<18} {'Searches/sec':<15} {'Batch'}\")\n",
    "print(\"-\" * 80)\n",
    "for r in mcts_results:\n",
    "    print(f\"{r['game']:<12} {r['device']:<8} {r['avg_sec']:>8.3f} ¬± {r['std_sec']:<6.3f} {r['searches_per_sec']:>10.1f}      {r['batch_size']:>2}\")\n",
    "\n",
    "print(\"\\n3. GAME GENERATION THROUGHPUT\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Game':<12} {'Device':<8} {'Time/Game (s)':<16} {'Games/Hour':<15} {'Avg Moves'}\")\n",
    "print(\"-\" * 80)\n",
    "for r in game_gen_results:\n",
    "    print(f\"{r['game']:<12} {r['device']:<8} {r['avg_time']:>10.2f}       {r['games_per_hour']:>10.0f}      {r['avg_moves']:>6.1f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY INSIGHTS:\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best performers\n",
    "if forward_results:\n",
    "    best_forward = min(forward_results, key=lambda x: x['avg_ms'])\n",
    "    print(f\"\\n‚úÖ Fastest forward pass: {best_forward['game']} on {best_forward['device']} ({best_forward['avg_ms']:.2f} ms)\")\n",
    "\n",
    "if mcts_results:\n",
    "    best_mcts = min(mcts_results, key=lambda x: x['avg_sec'])\n",
    "    print(f\"‚úÖ Fastest MCTS search: {best_mcts['game']} on {best_mcts['device']} ({best_mcts['searches_per_sec']:.1f} searches/sec)\")\n",
    "\n",
    "if game_gen_results:\n",
    "    best_gen = max(game_gen_results, key=lambda x: x['games_per_hour'])\n",
    "    print(f\"‚úÖ Highest throughput: {best_gen['game']} on {best_gen['device']} ({best_gen['games_per_hour']:.0f} games/hour)\")\n",
    "\n",
    "print(\"\\nüìä Recommendations:\")\n",
    "if 'cuda' in devices:\n",
    "    print(\"  ‚Ä¢ Use CUDA for training (best performance)\")\n",
    "    print(\"  ‚Ä¢ Use batch_size=16 for optimal GPU utilization\")\n",
    "elif 'mps' in devices:\n",
    "    print(\"  ‚Ä¢ Use MPS on Apple Silicon (good performance)\")\n",
    "    print(\"  ‚Ä¢ Batching provides moderate speedup on MPS\")\n",
    "else:\n",
    "    print(\"  ‚Ä¢ CPU only - consider reducing num_workers and batch_size\")\n",
    "    print(\"  ‚Ä¢ Training will be slower; use simpler games (TicTacToe)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Export Results\n",
    "\n",
    "Save benchmark results for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to file\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'devices': devices,\n",
    "    'forward_pass': forward_results,\n",
    "    'mcts_search': mcts_results,\n",
    "    'game_generation': game_gen_results,\n",
    "    'scaling': scaling_results if 'scaling_results' in dir() else [],\n",
    "    'batching': batching_results if 'batching_results' in dir() else []\n",
    "}\n",
    "\n",
    "output_file = f\"benchmark_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Results saved to: {output_file}\")\n",
    "print(f\"\\nFile size: {Path(output_file).stat().st_size / 1024:.1f} KB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
