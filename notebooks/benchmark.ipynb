{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tako HRM - MCTS Benchmark\n",
    "\n",
    "Benchmark MCTS search performance across different configurations and games.\n",
    "\n",
    "## What This Measures\n",
    "\n",
    "- **MCTS searches/second** - How fast can we search game trees?\n",
    "- **Forward pass time** - Neural network inference speed\n",
    "- **Batching efficiency** - Speedup from batched evaluation\n",
    "- **GPU vs CPU** - Device comparison\n",
    "- **Game complexity** - How game size affects performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Setup\n",
    "\n",
    "**Run `setup.ipynb` first if you haven't already!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FlashAttention (SDPA) enabled\n",
      "‚úÖ CUDA GPU: Tesla T4\n",
      "   Memory: 15.6 GB\n",
      "‚úÖ CPU\n",
      "\n",
      "‚úÖ Setup verified - ready to benchmark!\n",
      "Devices to test: ['cuda', 'cpu']\n"
     ]
    }
   ],
   "source": [
    "# Verify setup and import libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import yaml\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.chdir('/content/tako-v2')\n",
    "\n",
    "if not os.path.exists('scripts/train.py'):\n",
    "    print(\"‚ùå ERROR: Not in tako-v2 directory\")\n",
    "    print(\"   Run setup.ipynb first!\")\n",
    "    raise FileNotFoundError(\"Run setup.ipynb first\")\n",
    "\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "from model.hrm import HRM\n",
    "from training.mcts import MCTS\n",
    "from games.tictactoe import TicTacToeGame\n",
    "from games.othello import OthelloGame\n",
    "\n",
    "# Detect available devices\n",
    "devices = []\n",
    "if torch.cuda.is_available():\n",
    "    devices.append('cuda')\n",
    "    print(f\"‚úÖ CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "if torch.backends.mps.is_available():\n",
    "    devices.append('mps')\n",
    "    print(f\"‚úÖ Apple MPS\")\n",
    "devices.append('cpu')\n",
    "print(f\"‚úÖ CPU\")\n",
    "\n",
    "print(f\"\\n‚úÖ Setup verified - ready to benchmark!\")\n",
    "print(f\"Devices to test: {devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available devices\n",
    "import torch\n",
    "\n",
    "devices = []\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    devices.append('cuda')\n",
    "    print(f\"‚úÖ CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    devices.append('mps')\n",
    "    print(f\"‚úÖ Apple MPS (Metal Performance Shaders)\")\n",
    "if devices == []:\n",
    "    devices.append('cpu')\n",
    "    print(f\"‚úÖ CPU\")\n",
    "\n",
    "print(f\"\\nDevices to benchmark: {devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_forward_pass(game_name, device, num_trials=100, use_optimizations=False):\n",
    "    \"\"\"Benchmark forward pass speed with optional optimizations.\n",
    "    \n",
    "    Args:\n",
    "        game_name: Game to benchmark ('tictactoe', 'othello')\n",
    "        device: Device to use ('cuda', 'cpu', 'mps')\n",
    "        num_trials: Number of trials to run\n",
    "        use_optimizations: Enable torch.compile + bfloat16 + inference_mode (default: True)\n",
    "    \"\"\"\n",
    "    # Load config\n",
    "    with open(f'config/{game_name}.yaml') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Create model\n",
    "    model = HRM(**config['model'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Apply optimizations if requested\n",
    "    if use_optimizations:\n",
    "        if device == 'cuda':\n",
    "            dtype = torch.bfloat16\n",
    "            use_compile = True\n",
    "        else:\n",
    "            dtype = None  # Keep float32 on CPU/MPS\n",
    "            use_compile = False  # torch.compile less beneficial on CPU\n",
    "        \n",
    "        if dtype is not None or use_compile:\n",
    "            model.optimize_for_inference(use_compile=use_compile, dtype=None)\n",
    "    \n",
    "    # Create game for tokens\n",
    "    if game_name == 'tictactoe':\n",
    "        game = TicTacToeGame()\n",
    "    elif game_name == 'othello':\n",
    "        game = OthelloGame()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown game: {game_name}\")\n",
    "    \n",
    "    tokens = game.to_tokens().unsqueeze(0).to(device)\n",
    "    max_segments = config['mcts'].get('max_segments_inference', 1)\n",
    "    \n",
    "    # Warmup (extra warmup for torch.compile first-run compilation)\n",
    "    warmup_iterations = 20 if use_optimizations else 10\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup_iterations):\n",
    "            policy, value, _ = model.predict(tokens, use_act=True, max_segments=max_segments)\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm(range(num_trials), desc=f\"{game_name} on {device}\"):\n",
    "            start = time.time()\n",
    "            policy, value, _ = model.predict(tokens, use_act=True, max_segments=max_segments)\n",
    "            if device == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            times.append(time.time() - start)\n",
    "    \n",
    "    avg_time = np.mean(times) * 1000  # Convert to ms\n",
    "    std_time = np.std(times) * 1000\n",
    "    \n",
    "    return {\n",
    "        'game': game_name,\n",
    "        'device': device,\n",
    "        'avg_ms': avg_time,\n",
    "        'std_ms': std_time,\n",
    "        'params': sum(p.numel() for p in model.parameters()) / 1e6,\n",
    "        'max_segments': max_segments,\n",
    "        'optimized': use_optimizations\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Forward pass benchmark function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run forward pass benchmarks\n",
    "forward_results = []\n",
    "\n",
    "games_to_test = ['tictactoe', 'othello']\n",
    "\n",
    "for game in games_to_test:\n",
    "    for device in devices:\n",
    "        try:\n",
    "            result = benchmark_forward_pass(game, device, num_trials=100)\n",
    "            forward_results.append(result)\n",
    "            print(f\"\\n{game} on {device}: {result['avg_ms']:.2f} ¬± {result['std_ms']:.2f} ms\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è  {game} on {device} failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Forward Pass Benchmark Results:\")\n",
    "print(\"=\"*80)\n",
    "for r in forward_results:\n",
    "    print(f\"{r['game']:<12} {r['device']:<6} {r['avg_ms']:>7.2f} ms  ({r['params']:.1f}M params, {r['max_segments']} seg)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Benchmark 2: MCTS Search Speed\n",
    "\n",
    "Measure complete MCTS search including tree traversal + neural evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_mcts_search(game_name, device, num_searches=50, simulations=25):\n",
    "    \"\"\"Benchmark MCTS search speed.\"\"\"\n",
    "    # Load config\n",
    "    with open(f'config/{game_name}.yaml') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Override simulations\n",
    "    config['mcts']['simulations'] = simulations\n",
    "    \n",
    "    # Create model\n",
    "    model = HRM(**config['model'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create game class\n",
    "    if game_name == 'tictactoe':\n",
    "        game_class = TicTacToeGame\n",
    "    elif game_name == 'othello':\n",
    "        game_class = OthelloGame\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown game: {game_name}\")\n",
    "    \n",
    "    # Create MCTS\n",
    "    mcts = MCTS(model, game_class, config['mcts'], device=device)\n",
    "    \n",
    "    # Warmup\n",
    "    game = game_class()\n",
    "    for _ in range(5):\n",
    "        _ = mcts.search(game, move_num=0)\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    for _ in tqdm(range(num_searches), desc=f\"{game_name} MCTS on {device}\"):\n",
    "        game = game_class()\n",
    "        start = time.time()\n",
    "        policy = mcts.search(game, move_num=0)  # Returns policy distribution\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    \n",
    "    return {\n",
    "        'game': game_name,\n",
    "        'device': device,\n",
    "        'simulations': simulations,\n",
    "        'avg_sec': avg_time,\n",
    "        'std_sec': std_time,\n",
    "        'searches_per_sec': 1.0 / avg_time,\n",
    "        'batch_size': config['mcts'].get('batch_size', 1)\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ MCTS benchmark function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MCTS benchmarks with default config\n",
    "mcts_results = []\n",
    "\n",
    "games_to_test = ['tictactoe', 'othello']\n",
    "\n",
    "for game in games_to_test:\n",
    "    for device in devices:\n",
    "        try:\n",
    "            result = benchmark_mcts_search(game, device, num_searches=50, simulations=25)\n",
    "            mcts_results.append(result)\n",
    "            print(f\"\\n{game} on {device}:\")\n",
    "            print(f\"  {result['avg_sec']:.3f} ¬± {result['std_sec']:.3f} sec/search\")\n",
    "            print(f\"  {result['searches_per_sec']:.1f} searches/sec\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è  {game} on {device} failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MCTS Search Benchmark Results (25 simulations):\")\n",
    "print(\"=\"*80)\n",
    "for r in mcts_results:\n",
    "    print(f\"{r['game']:<12} {r['device']:<6} {r['avg_sec']:>6.3f} s  ({r['searches_per_sec']:>6.1f} searches/s, batch={r['batch_size']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Benchmark 3: Scaling with Simulations\n",
    "\n",
    "How does MCTS performance scale with different numbers of simulations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark MCTS with different simulation counts\n",
    "GAME = \"tictactoe\"  # Change to 'othello' for larger game\n",
    "DEVICE = devices[0]  # Use best available device\n",
    "\n",
    "simulation_counts = [10, 25, 50, 100, 200, 400]\n",
    "scaling_results = []\n",
    "\n",
    "print(f\"Benchmarking {GAME} on {DEVICE} with varying simulations...\\n\")\n",
    "\n",
    "for sims in simulation_counts:\n",
    "    result = benchmark_mcts_search(GAME, DEVICE, num_searches=30, simulations=sims)\n",
    "    scaling_results.append(result)\n",
    "    print(f\"{sims:>4} sims: {result['avg_sec']:.3f} s ({result['searches_per_sec']:.1f} searches/s)\")\n",
    "\n",
    "print(\"\\n‚úÖ Scaling benchmark complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scaling results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "sims = [r['simulations'] for r in scaling_results]\n",
    "times = [r['avg_sec'] for r in scaling_results]\n",
    "throughput = [r['searches_per_sec'] for r in scaling_results]\n",
    "\n",
    "# Time vs simulations\n",
    "axes[0].plot(sims, times, 'o-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('MCTS Simulations', fontsize=12)\n",
    "axes[0].set_ylabel('Time per Search (seconds)', fontsize=12)\n",
    "axes[0].set_title(f'{GAME.capitalize()} - Search Time vs Simulations', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xscale('log')\n",
    "\n",
    "# Throughput vs simulations\n",
    "axes[1].plot(sims, throughput, 'o-', linewidth=2, markersize=8, color='green')\n",
    "axes[1].set_xlabel('MCTS Simulations', fontsize=12)\n",
    "axes[1].set_ylabel('Searches per Second', fontsize=12)\n",
    "axes[1].set_title(f'{GAME.capitalize()} - Throughput vs Simulations', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Benchmark 4: Batching Efficiency\n",
    "\n",
    "Compare batched vs non-batched MCTS evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_batching(game_name, device, batch_sizes=[1, 4, 8, 16, 32]):\n",
    "    \"\"\"Benchmark different batch sizes.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        # Load config\n",
    "        with open(f'config/{game_name}.yaml') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        \n",
    "        # Override batch size\n",
    "        config['mcts']['batch_size'] = batch_size\n",
    "        config['mcts']['simulations'] = 100  # Fixed for comparison\n",
    "        \n",
    "        # Create model\n",
    "        model = HRM(**config['model'])\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        # Create game class\n",
    "        if game_name == 'tictactoe':\n",
    "            game_class = TicTacToeGame\n",
    "        elif game_name == 'othello':\n",
    "            game_class = OthelloGame\n",
    "        \n",
    "        # Create MCTS\n",
    "        mcts = MCTS(model, game_class, config['mcts'], device=device)\n",
    "        \n",
    "        # Warmup\n",
    "        game = game_class()\n",
    "        for _ in range(3):\n",
    "            _ = mcts.search(game, move_num=0)\n",
    "        \n",
    "        # Benchmark\n",
    "        times = []\n",
    "        for _ in range(20):\n",
    "            game = game_class()\n",
    "            start = time.time()\n",
    "            policy = mcts.search(game, move_num=0)  # Returns policy distribution\n",
    "            times.append(time.time() - start)\n",
    "        \n",
    "        avg_time = np.mean(times)\n",
    "        results.append({\n",
    "            'batch_size': batch_size,\n",
    "            'avg_sec': avg_time,\n",
    "            'searches_per_sec': 1.0 / avg_time\n",
    "        })\n",
    "        \n",
    "        print(f\"Batch size {batch_size:>2}: {avg_time:.3f} s ({1.0/avg_time:.1f} searches/s)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Batching benchmark function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run batching benchmark\n",
    "GAME = \"tictactoe\"\n",
    "DEVICE = devices[0]\n",
    "\n",
    "print(f\"Benchmarking batching for {GAME} on {DEVICE}...\\n\")\n",
    "batching_results = benchmark_batching(GAME, DEVICE, batch_sizes=[1, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048])\n",
    "\n",
    "print(\"\\n‚úÖ Batching benchmark complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot batching efficiency\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "batch_sizes = [r['batch_size'] for r in batching_results]\n",
    "times = [r['avg_sec'] for r in batching_results]\n",
    "throughput = [r['searches_per_sec'] for r in batching_results]\n",
    "\n",
    "# Speedup vs batch size\n",
    "baseline_time = batching_results[0]['avg_sec']  # batch_size=1\n",
    "speedups = [baseline_time / t for t in times]\n",
    "\n",
    "axes[0].plot(batch_sizes, speedups, 'o-', linewidth=2, markersize=8)\n",
    "axes[0].axhline(y=1, color='r', linestyle='--', alpha=0.5, label='No speedup')\n",
    "axes[0].set_xlabel('Batch Size', fontsize=12)\n",
    "axes[0].set_ylabel('Speedup vs Batch=1', fontsize=12)\n",
    "axes[0].set_title(f'{GAME.capitalize()} - Batching Speedup', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "axes[0].set_xscale('log', base=2)\n",
    "\n",
    "# Efficiency (speedup / batch_size)\n",
    "efficiency = [speedup / bs for speedup, bs in zip(speedups, batch_sizes)]\n",
    "axes[1].plot(batch_sizes, efficiency, 'o-', linewidth=2, markersize=8, color='orange')\n",
    "axes[1].axhline(y=1, color='r', linestyle='--', alpha=0.5, label='Perfect scaling')\n",
    "axes[1].set_xlabel('Batch Size', fontsize=12)\n",
    "axes[1].set_ylabel('Batching Efficiency', fontsize=12)\n",
    "axes[1].set_title(f'{GAME.capitalize()} - Batching Efficiency', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "axes[1].set_xscale('log', base=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Best batch size: {batch_sizes[speedups.index(max(speedups))]} ({max(speedups):.1f}x speedup)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Benchmark 5: End-to-End Game Generation\n",
    "\n",
    "Measure complete game generation time (full playthrough)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_game_generation(game_name, device, num_games=20):\n",
    "    \"\"\"Benchmark full game generation.\"\"\"\n",
    "    # Load config\n",
    "    with open(f'config/{game_name}.yaml') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    config['mcts']['batch_size'] = 2048 \n",
    "    # Create model\n",
    "    model = HRM(**config['model'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create game class\n",
    "    if game_name == 'tictactoe':\n",
    "        game_class = TicTacToeGame\n",
    "    elif game_name == 'othello':\n",
    "        game_class = OthelloGame\n",
    "    \n",
    "    # Create MCTS\n",
    "    mcts = MCTS(model, game_class, config['mcts'], device=device)\n",
    "    \n",
    "    # Generate games\n",
    "    times = []\n",
    "    move_counts = []\n",
    "    \n",
    "    for _ in tqdm(range(num_games), desc=f\"Generating {game_name} games\"):\n",
    "        game = game_class()\n",
    "        move_num = 0\n",
    "        \n",
    "        start = time.time()\n",
    "        while not game.is_terminal():\n",
    "            policy = mcts.search(game, move_num=move_num)  # Returns policy distribution\n",
    "            # Select move from policy\n",
    "            legal_moves = game.legal_moves()\n",
    "            legal_policy = policy[legal_moves]\n",
    "            move = legal_moves[np.argmax(legal_policy)]\n",
    "            game.make_move(move)\n",
    "            move_num += 1\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        times.append(elapsed)\n",
    "        move_counts.append(move_num)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    avg_moves = np.mean(move_counts)\n",
    "    games_per_hour = 3600 / avg_time\n",
    "    \n",
    "    return {\n",
    "        'game': game_name,\n",
    "        'device': device,\n",
    "        'avg_time': avg_time,\n",
    "        'avg_moves': avg_moves,\n",
    "        'games_per_hour': games_per_hour,\n",
    "        'simulations': config['mcts']['simulations']\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Game generation benchmark function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run game generation benchmarks\n",
    "game_gen_results = []\n",
    "\n",
    "games_to_test = ['tictactoe', 'othello']\n",
    "\n",
    "for game in games_to_test:\n",
    "    for device in devices:\n",
    "        try:\n",
    "            result = benchmark_game_generation(game, device, num_games=20)\n",
    "            game_gen_results.append(result)\n",
    "            print(f\"\\n{game} on {device}:\")\n",
    "            print(f\"  {result['avg_time']:.2f} sec/game ({result['avg_moves']:.1f} moves avg)\")\n",
    "            print(f\"  {result['games_per_hour']:.0f} games/hour\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è  {game} on {device} failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Game Generation Benchmark Results:\")\n",
    "print(\"=\"*80)\n",
    "for r in game_gen_results:\n",
    "    print(f\"{r['game']:<12} {r['device']:<6} {r['avg_time']:>6.2f} s/game  ({r['games_per_hour']:>7.0f} games/hr, {r['simulations']} sims)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Benchmark 6: Ray Worker Scaling (Single GPU)\n",
    "\n",
    "Find the optimal number of Ray workers for self-play when sharing a single GPU.\n",
    "\n",
    "**What this measures:**\n",
    "- Self-play throughput (games/hour) vs number of workers\n",
    "- GPU utilization and bottlenecks\n",
    "- CPU vs GPU bound regions\n",
    "- Optimal worker count for maximum throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ray worker benchmark function ready\n"
     ]
    }
   ],
   "source": [
    "from training.worker import SelfPlayWorker\n",
    "import games\n",
    "import os\n",
    "\n",
    "def benchmark_ray_workers(game_name, worker_counts, games_per_worker=5, device='cuda'):\n",
    "    \"\"\"Benchmark self-play throughput vs number of Ray workers.\"\"\"\n",
    "    with open(f'config/{game_name}.yaml') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    results = []\n",
    "    num_gpus = 1\n",
    "    for num_workers in worker_counts:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing {num_workers} workers...\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Distribute both GPU and CPU fractions evenly so Ray never gates\n",
    "        # scheduling on whole-resource availability.\n",
    "        cpu_fraction = (os.cpu_count() or 1) / num_workers\n",
    "        if device == 'cuda':\n",
    "            gpu_fraction = num_gpus / num_workers\n",
    "\n",
    "        workers = []\n",
    "        for i in range(num_workers):\n",
    "            if device == 'cuda':\n",
    "                worker_device = f'cuda:{i % num_gpus}'\n",
    "                worker = SelfPlayWorker.options(num_gpus=gpu_fraction, num_cpus=cpu_fraction).remote(\n",
    "                    worker_id=i,\n",
    "                    game_class=games.TicTacToeGame,\n",
    "                    model_config=config['model'],\n",
    "                    mcts_config=config['mcts'],\n",
    "                    opponent_pool_config={'recent_weight': 0},\n",
    "                    device=worker_device\n",
    "                )\n",
    "            else:\n",
    "                worker = SelfPlayWorker.options(num_cpus=cpu_fraction).remote(\n",
    "                    worker_id=i,\n",
    "                    game_class=games.TicTacToeGame,\n",
    "                    model_config=config['model'],\n",
    "                    mcts_config=config['mcts'],\n",
    "                    opponent_pool_config={'recent_weight': 0},\n",
    "                    device=device\n",
    "                )\n",
    "            workers.append(worker)\n",
    "            if device == 'cuda':\n",
    "                print(f\"  Worker {i}: gpu={gpu_fraction:.4f} cpu={cpu_fraction:.4f}\")\n",
    "            else:\n",
    "                print(f\"  Worker {i}: {device} cpu={cpu_fraction:.4f}\")\n",
    "\n",
    "        # Warmup\n",
    "        ray.get([w.generate_game.remote() for w in workers])\n",
    "        print(f\"  Warmup complete\")\n",
    "        \n",
    "        # Benchmark\n",
    "        start_time = time.time()\n",
    "        worker_results = ray.get([worker.generate_batch.remote(games_per_worker) for worker in workers])\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        total_games = num_workers * games_per_worker\n",
    "        total_positions = sum(len(samples) for samples in worker_results)\n",
    "        avg_moves = total_positions / total_games if total_games > 0 else 0\n",
    "        games_per_sec = total_games / elapsed if elapsed > 0 else 0\n",
    "\n",
    "        result = {\n",
    "            'num_workers': num_workers,\n",
    "            'total_games': total_games,\n",
    "            'elapsed_sec': elapsed,\n",
    "            'games_per_sec': games_per_sec,\n",
    "            'games_per_hour': games_per_sec * 3600,\n",
    "            'positions_per_sec': total_positions / elapsed if elapsed > 0 else 0,\n",
    "            'avg_moves': avg_moves\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"  Total games: {total_games}\")\n",
    "        print(f\"  Time: {elapsed:.1f}s\")\n",
    "        print(f\"  Throughput: {games_per_sec:.2f} games/s ({result['games_per_hour']:.0f} games/hr)\")\n",
    "        print(f\"  Positions/sec: {result['positions_per_sec']:.1f}\")\n",
    "        \n",
    "        for worker in workers:\n",
    "            ray.kill(worker)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Ray worker benchmark function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 22:43:49,061\tINFO worker.py:2013 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ray initialized\n",
      "   CPU cores: 2\n",
      "   Available memory: 10.7 GB\n",
      "Benchmarking Ray workers for tictactoe on cuda\n",
      "Worker counts to test: [1, 2, 4, 8, 16, 32]\n",
      "Each worker will generate 5 games per test\n",
      "\n",
      "\n",
      "============================================================\n",
      "Testing 1 workers...\n",
      "============================================================\n",
      "  Worker 0: gpu=1.0000 cpu=2.0000\n",
      "\u001b[36m(SelfPlayWorker pid=16094)\u001b[0m ‚úÖ FlashAttention (SDPA) enabled\n",
      "  Warmup complete\n",
      "  Total games: 5\n",
      "  Time: 1.8s\n",
      "  Throughput: 2.79 games/s (10035 games/hr)\n",
      "  Positions/sec: 11.1\n",
      "\n",
      "============================================================\n",
      "Testing 2 workers...\n",
      "============================================================\n",
      "  Worker 0: gpu=0.5000 cpu=1.0000\n",
      "  Worker 1: gpu=0.5000 cpu=1.0000\n",
      "\u001b[36m(SelfPlayWorker pid=16205)\u001b[0m ‚úÖ FlashAttention (SDPA) enabled\n",
      "\u001b[36m(SelfPlayWorker pid=16206)\u001b[0m ‚úÖ FlashAttention (SDPA) enabled\n",
      "  Warmup complete\n",
      "  Total games: 10\n",
      "  Time: 3.4s\n",
      "  Throughput: 2.93 games/s (10556 games/hr)\n",
      "  Positions/sec: 12.6\n",
      "\n",
      "============================================================\n",
      "Testing 4 workers...\n",
      "============================================================\n",
      "  Worker 0: gpu=0.2500 cpu=0.5000\n",
      "  Worker 1: gpu=0.2500 cpu=0.5000\n",
      "  Worker 2: gpu=0.2500 cpu=0.5000\n",
      "  Worker 3: gpu=0.2500 cpu=0.5000\n",
      "\u001b[36m(SelfPlayWorker pid=16319)\u001b[0m ‚úÖ FlashAttention (SDPA) enabled\n",
      "\u001b[36m(SelfPlayWorker pid=16318)\u001b[0m ‚úÖ FlashAttention (SDPA) enabled\n",
      "  Warmup complete\n",
      "  Total games: 20\n",
      "  Time: 7.0s\n",
      "  Throughput: 2.85 games/s (10274 games/hr)\n",
      "  Positions/sec: 12.8\n",
      "\n",
      "============================================================\n",
      "Testing 8 workers...\n",
      "============================================================\n",
      "  Worker 0: gpu=0.1250 cpu=0.2500\n",
      "  Worker 1: gpu=0.1250 cpu=0.2500\n",
      "  Worker 2: gpu=0.1250 cpu=0.2500\n",
      "  Worker 3: gpu=0.1250 cpu=0.2500\n",
      "  Worker 4: gpu=0.1250 cpu=0.2500\n",
      "  Worker 5: gpu=0.1250 cpu=0.2500\n",
      "  Worker 6: gpu=0.1250 cpu=0.2500\n",
      "  Worker 7: gpu=0.1250 cpu=0.2500\n",
      "\u001b[36m(SelfPlayWorker pid=16587)\u001b[0m ‚úÖ FlashAttention (SDPA) enabled\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[36m(SelfPlayWorker pid=16763)\u001b[0m ‚úÖ FlashAttention (SDPA) enabled\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "  Warmup complete\n",
      "  Total games: 40\n",
      "  Time: 13.4s\n",
      "  Throughput: 3.00 games/s (10784 games/hr)\n",
      "  Positions/sec: 13.2\n",
      "\n",
      "============================================================\n",
      "Testing 16 workers...\n",
      "============================================================\n",
      "  Worker 0: gpu=0.0625 cpu=0.1250\n",
      "  Worker 1: gpu=0.0625 cpu=0.1250\n",
      "  Worker 2: gpu=0.0625 cpu=0.1250\n",
      "  Worker 3: gpu=0.0625 cpu=0.1250\n",
      "  Worker 4: gpu=0.0625 cpu=0.1250\n",
      "  Worker 5: gpu=0.0625 cpu=0.1250\n",
      "  Worker 6: gpu=0.0625 cpu=0.1250\n",
      "  Worker 7: gpu=0.0625 cpu=0.1250\n",
      "  Worker 8: gpu=0.0625 cpu=0.1250\n",
      "  Worker 9: gpu=0.0625 cpu=0.1250\n",
      "  Worker 10: gpu=0.0625 cpu=0.1250\n",
      "  Worker 11: gpu=0.0625 cpu=0.1250\n",
      "  Worker 12: gpu=0.0625 cpu=0.1250\n",
      "  Worker 13: gpu=0.0625 cpu=0.1250\n",
      "  Worker 14: gpu=0.0625 cpu=0.1250\n",
      "  Worker 15: gpu=0.0625 cpu=0.1250\n",
      "\u001b[33m(raylet)\u001b[0m WARNING: 8 PYTHON worker processes have been started on node: 3239ea4cca05aa2aa0da395b856225b9a36d5f9e430ac65c84a62713 with address: 172.28.0.12. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\n",
      "\u001b[33m(raylet)\u001b[0m WARNING: 14 PYTHON worker processes have been started on node: 3239ea4cca05aa2aa0da395b856225b9a36d5f9e430ac65c84a62713 with address: 172.28.0.12. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
      "\u001b[33m(raylet)\u001b[0m WARNING: 18 PYTHON worker processes have been started on node: 3239ea4cca05aa2aa0da395b856225b9a36d5f9e430ac65c84a62713 with address: 172.28.0.12. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(SelfPlayWorker pid=17008)\u001b[0m ‚úÖ FlashAttention (SDPA) enabled\n",
      "\u001b[33m(raylet)\u001b[0m WARNING: 20 PYTHON worker processes have been started on node: 3239ea4cca05aa2aa0da395b856225b9a36d5f9e430ac65c84a62713 with address: 172.28.0.12. This could be a result of using a large number of actors, or due to tasks blocked in ray.get() calls (see https://github.com/ray-project/ray/issues/3644 for some discussion of workarounds).\n",
      "\u001b[36m(SelfPlayWorker pid=17118)\u001b[0m ‚úÖ FlashAttention (SDPA) enabled\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
      "\u001b[36m(SelfPlayWorker pid=17437)\u001b[0m ‚úÖ FlashAttention (SDPA) enabled\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
      "\n",
      "‚ö†Ô∏è  Ray worker benchmark failed: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 3239ea4cca05aa2aa0da395b856225b9a36d5f9e430ac65c84a62713) where the lease (lease ID: 1e000000ce17c84e55369086d084f16630f97a2f4801f02f241731e0b6d293da, name=SelfPlayWorker.__init__, pid=17518, memory used=0.47GB) was running was 12.18GB / 12.67GB (0.961106), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4f2d51c2a6f80f47e7bb5b81e7f8e477b3d488797e1e2669056e80ab) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4f2d51c2a6f80f47e7bb5b81e7f8e477b3d488797e1e2669056e80ab*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "17299\t0.64\tray::SelfPlayWorker\n",
      "17143\t0.64\tray::SelfPlayWorker\n",
      "17118\t0.64\tray::SelfPlayWorker\n",
      "17437\t0.64\tray::SelfPlayWorker.generate_game\n",
      "17007\t0.64\tray::SelfPlayWorker\n",
      "17046\t0.64\tray::SelfPlayWorker\n",
      "17077\t0.64\tray::SelfPlayWorker\n",
      "17008\t0.64\tray::SelfPlayWorker\n",
      "17215\t0.64\tray::SelfPlayWorker.generate_game\n",
      "17192\t0.64\tray::SelfPlayWorker.generate_game\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipython-input-984/1583391457.py\", line 36, in <cell line: 0>\n",
      "    ray_worker_results = benchmark_ray_workers(\n",
      "                         ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipython-input-984/1493518423.py\", line 51, in benchmark_ray_workers\n",
      "    ray.get([w.generate_game.remote() for w in workers])\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 22, in auto_init_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 104, in wrapper\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2981, in get\n",
      "    values, debugger_breakpoint = worker.get_objects(\n",
      "                                  ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 1014, in get_objects\n",
      "    raise value\n",
      "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
      "Memory on the node (IP: 172.28.0.12, ID: 3239ea4cca05aa2aa0da395b856225b9a36d5f9e430ac65c84a62713) where the lease (lease ID: 1e000000ce17c84e55369086d084f16630f97a2f4801f02f241731e0b6d293da, name=SelfPlayWorker.__init__, pid=17518, memory used=0.47GB) was running was 12.18GB / 12.67GB (0.961106), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 4f2d51c2a6f80f47e7bb5b81e7f8e477b3d488797e1e2669056e80ab) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-4f2d51c2a6f80f47e7bb5b81e7f8e477b3d488797e1e2669056e80ab*out -ip 172.28.0.12. Top 10 memory users:\n",
      "PID\tMEM(GB)\tCOMMAND\n",
      "17299\t0.64\tray::SelfPlayWorker\n",
      "17143\t0.64\tray::SelfPlayWorker\n",
      "17118\t0.64\tray::SelfPlayWorker\n",
      "17437\t0.64\tray::SelfPlayWorker.generate_game\n",
      "17007\t0.64\tray::SelfPlayWorker\n",
      "17046\t0.64\tray::SelfPlayWorker\n",
      "17077\t0.64\tray::SelfPlayWorker\n",
      "17008\t0.64\tray::SelfPlayWorker\n",
      "17215\t0.64\tray::SelfPlayWorker.generate_game\n",
      "17192\t0.64\tray::SelfPlayWorker.generate_game\n",
      "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m [2026-02-25 22:46:49,005 E 15985 15985] (raylet) node_manager.cc:3250: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: 3239ea4cca05aa2aa0da395b856225b9a36d5f9e430ac65c84a62713, IP: 172.28.0.12) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.28.0.12`\n",
      "\u001b[33m(raylet)\u001b[0m \n",
      "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
     ]
    }
   ],
   "source": [
    "# Run Ray worker scaling benchmark\n",
    "\n",
    "import ray\n",
    "import psutil\n",
    "import multiprocessing as mp\n",
    "\n",
    "# Check Ray availability\n",
    "for _ in range(2):\n",
    "    try:\n",
    "        if not ray.is_initialized():\n",
    "            ray.init(ignore_reinit_error=True)\n",
    "        print(f\"‚úÖ Ray initialized\")\n",
    "        print(f\"   CPU cores: {psutil.cpu_count()}\")\n",
    "        print(f\"   Available memory: {psutil.virtual_memory().available / 1e9:.1f} GB\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        %pip install ray\n",
    "        print(f\"‚ö†Ô∏è  Ray not available: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "GAME = \"tictactoe\"  # Use tictactoe for faster iteration\n",
    "DEVICE = 'cuda' if 'cuda' in devices else 'cpu'\n",
    "\n",
    "# Test worker counts: 1, 2, 4, 8, 16, 32\n",
    "# Adjust based on available CPU cores\n",
    "# max_workers = min(32, psutil.cpu_count())\n",
    "worker_counts = [1, 2, 4, 8, 16, 32]\n",
    "# worker_counts = [w for w in worker_counts if w <= max_workers]\n",
    "\n",
    "print(f\"Benchmarking Ray workers for {GAME} on {DEVICE}\")\n",
    "print(f\"Worker counts to test: {worker_counts}\")\n",
    "print(f\"Each worker will generate 5 games per test\\n\")\n",
    "\n",
    "try:\n",
    "    ray_worker_results = benchmark_ray_workers(\n",
    "        game_name=GAME,\n",
    "        worker_counts=worker_counts,\n",
    "        games_per_worker=5,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RAY WORKER SCALING RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Workers':<10} {'Games/sec':<12} {'Games/hour':<15} {'Positions/sec':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    for r in ray_worker_results:\n",
    "        print(f\"{r['num_workers']:<10} {r['games_per_sec']:<12.2f} {r['games_per_hour']:<15.0f} {r['positions_per_sec']:<15.1f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Ray worker benchmark complete\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Ray worker benchmark failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    ray_worker_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Ray worker scaling results\n",
    "if ray_worker_results:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    workers = [r['num_workers'] for r in ray_worker_results]\n",
    "    throughput = [r['games_per_hour'] for r in ray_worker_results]\n",
    "    positions_per_sec = [r['positions_per_sec'] for r in ray_worker_results]\n",
    "    \n",
    "    # Calculate speedup vs single worker\n",
    "    baseline_throughput = ray_worker_results[0]['games_per_hour']\n",
    "    speedups = [t / baseline_throughput for t in throughput]\n",
    "    efficiency = [speedup / w for speedup, w in zip(speedups, workers)]\n",
    "    \n",
    "    # Plot 1: Throughput vs workers\n",
    "    axes[0].plot(workers, throughput, 'o-', linewidth=2, markersize=8, color='blue')\n",
    "    axes[0].set_xlabel('Number of Workers', fontsize=12)\n",
    "    axes[0].set_ylabel('Games per Hour', fontsize=12)\n",
    "    axes[0].set_title(f'{GAME.capitalize()} - Self-Play Throughput', fontsize=14)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_xscale('log', base=2)\n",
    "    \n",
    "    # Plot 2: Speedup vs workers\n",
    "    axes[1].plot(workers, speedups, 'o-', linewidth=2, markersize=8, color='green')\n",
    "    axes[1].plot(workers, workers, '--', alpha=0.5, color='red', label='Linear scaling')\n",
    "    axes[1].set_xlabel('Number of Workers', fontsize=12)\n",
    "    axes[1].set_ylabel('Speedup vs 1 Worker', fontsize=12)\n",
    "    axes[1].set_title(f'{GAME.capitalize()} - Scaling Efficiency', fontsize=14)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].legend()\n",
    "    axes[1].set_xscale('log', base=2)\n",
    "    axes[1].set_yscale('log', base=2)\n",
    "    \n",
    "    # Plot 3: Parallel efficiency\n",
    "    axes[2].plot(workers, efficiency, 'o-', linewidth=2, markersize=8, color='orange')\n",
    "    axes[2].axhline(y=1, color='r', linestyle='--', alpha=0.5, label='Perfect efficiency')\n",
    "    axes[2].axhline(y=0.8, color='orange', linestyle='--', alpha=0.5, label='80% efficiency')\n",
    "    axes[2].set_xlabel('Number of Workers', fontsize=12)\n",
    "    axes[2].set_ylabel('Parallel Efficiency', fontsize=12)\n",
    "    axes[2].set_title(f'{GAME.capitalize()} - Worker Efficiency', fontsize=14)\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].legend()\n",
    "    axes[2].set_xscale('log', base=2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find optimal worker count (best efficiency above 80% or max throughput)\n",
    "    optimal_idx = -1\n",
    "    for i, (eff, w) in enumerate(zip(efficiency, workers)):\n",
    "        if eff >= 0.8:\n",
    "            optimal_idx = i\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    if optimal_idx >= 0:\n",
    "        optimal_workers = workers[optimal_idx]\n",
    "        optimal_throughput = throughput[optimal_idx]\n",
    "        optimal_speedup = speedups[optimal_idx]\n",
    "        \n",
    "        print(f\"\\nüìä Optimal Configuration:\")\n",
    "        print(f\"  Workers: {optimal_workers}\")\n",
    "        print(f\"  Throughput: {optimal_throughput:.0f} games/hour\")\n",
    "        print(f\"  Speedup: {optimal_speedup:.1f}x\")\n",
    "        print(f\"  Efficiency: {efficiency[optimal_idx]*100:.1f}%\")\n",
    "    else:\n",
    "        # Just use max throughput\n",
    "        max_idx = throughput.index(max(throughput))\n",
    "        print(f\"\\nüìä Best Performance:\")\n",
    "        print(f\"  Workers: {workers[max_idx]}\")\n",
    "        print(f\"  Throughput: {throughput[max_idx]:.0f} games/hour\")\n",
    "        print(f\"  Speedup: {speedups[max_idx]:.1f}x\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No Ray worker results to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Export Results\n",
    "\n",
    "Save benchmark results for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to file\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'devices': devices,\n",
    "    'forward_pass': forward_results,\n",
    "    'mcts_search': mcts_results,\n",
    "    'game_generation': game_gen_results,\n",
    "    'scaling': scaling_results if 'scaling_results' in dir() else [],\n",
    "    'batching': batching_results if 'batching_results' in dir() else [],\n",
    "    'ray_workers': ray_worker_results if 'ray_worker_results' in dir() and ray_worker_results else []\n",
    "}\n",
    "\n",
    "output_file = f\"benchmark_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Results saved to: {output_file}\")\n",
    "print(f\"\\nFile size: {Path(output_file).stat().st_size / 1024:.1f} KB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
