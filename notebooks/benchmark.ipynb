{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tako HRM - MCTS Benchmark\n",
    "\n",
    "Benchmark MCTS search performance across different configurations and games.\n",
    "\n",
    "## What This Measures\n",
    "\n",
    "- **MCTS searches/second** - How fast can we search game trees?\n",
    "- **Forward pass time** - Neural network inference speed\n",
    "- **Batching efficiency** - Speedup from batched evaluation\n",
    "- **GPU vs CPU** - Device comparison\n",
    "- **Game complexity** - How game size affects performance\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Setup\n",
    "\n",
    "**Run `setup.ipynb` first if you haven't already!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FlashAttention (SDPA) enabled\n",
      "‚úÖ CUDA GPU: Tesla T4\n",
      "   Memory: 15.6 GB\n",
      "‚úÖ CPU\n",
      "\n",
      "‚úÖ Setup verified - ready to benchmark!\n",
      "Devices to test: ['cuda', 'cpu']\n"
     ]
    }
   ],
   "source": [
    "# Verify setup and import libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import yaml\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.chdir('/content/tako-v2')\n",
    "\n",
    "if not os.path.exists('scripts/train.py'):\n",
    "    print(\"‚ùå ERROR: Not in tako-v2 directory\")\n",
    "    print(\"   Run setup.ipynb first!\")\n",
    "    raise FileNotFoundError(\"Run setup.ipynb first\")\n",
    "\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "from model.hrm import HRM\n",
    "from training.mcts import MCTS\n",
    "from games.tictactoe import TicTacToeGame\n",
    "from games.othello import OthelloGame\n",
    "\n",
    "# Detect available devices\n",
    "devices = []\n",
    "if torch.cuda.is_available():\n",
    "    devices.append('cuda')\n",
    "    print(f\"‚úÖ CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "if torch.backends.mps.is_available():\n",
    "    devices.append('mps')\n",
    "    print(f\"‚úÖ Apple MPS\")\n",
    "devices.append('cpu')\n",
    "print(f\"‚úÖ CPU\")\n",
    "\n",
    "print(f\"\\n‚úÖ Setup verified - ready to benchmark!\")\n",
    "print(f\"Devices to test: {devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available devices\n",
    "import torch\n",
    "\n",
    "devices = []\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    devices.append('cuda')\n",
    "    print(f\"‚úÖ CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    devices.append('mps')\n",
    "    print(f\"‚úÖ Apple MPS (Metal Performance Shaders)\")\n",
    "if devices == []:\n",
    "    devices.append('cpu')\n",
    "    print(f\"‚úÖ CPU\")\n",
    "\n",
    "print(f\"\\nDevices to benchmark: {devices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_forward_pass(game_name, device, num_trials=100, use_optimizations=False):\n",
    "    \"\"\"Benchmark forward pass speed with optional optimizations.\n",
    "    \n",
    "    Args:\n",
    "        game_name: Game to benchmark ('tictactoe', 'othello')\n",
    "        device: Device to use ('cuda', 'cpu', 'mps')\n",
    "        num_trials: Number of trials to run\n",
    "        use_optimizations: Enable torch.compile + bfloat16 + inference_mode (default: True)\n",
    "    \"\"\"\n",
    "    # Load config\n",
    "    with open(f'config/{game_name}.yaml') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Create model\n",
    "    model = HRM(**config['model'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Apply optimizations if requested\n",
    "    if use_optimizations:\n",
    "        if device == 'cuda':\n",
    "            dtype = torch.bfloat16\n",
    "            use_compile = True\n",
    "        else:\n",
    "            dtype = None  # Keep float32 on CPU/MPS\n",
    "            use_compile = False  # torch.compile less beneficial on CPU\n",
    "        \n",
    "        if dtype is not None or use_compile:\n",
    "            model.optimize_for_inference(use_compile=use_compile, dtype=None)\n",
    "    \n",
    "    # Create game for tokens\n",
    "    if game_name == 'tictactoe':\n",
    "        game = TicTacToeGame()\n",
    "    elif game_name == 'othello':\n",
    "        game = OthelloGame()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown game: {game_name}\")\n",
    "    \n",
    "    tokens = game.to_tokens().unsqueeze(0).to(device)\n",
    "    max_segments = config['mcts'].get('max_segments_inference', 1)\n",
    "    \n",
    "    # Warmup (extra warmup for torch.compile first-run compilation)\n",
    "    warmup_iterations = 20 if use_optimizations else 10\n",
    "    with torch.no_grad():\n",
    "        for _ in range(warmup_iterations):\n",
    "            policy, value, _ = model.predict(tokens, use_act=True, max_segments=max_segments)\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    with torch.no_grad():\n",
    "        for _ in tqdm(range(num_trials), desc=f\"{game_name} on {device}\"):\n",
    "            start = time.time()\n",
    "            policy, value, _ = model.predict(tokens, use_act=True, max_segments=max_segments)\n",
    "            if device == 'cuda':\n",
    "                torch.cuda.synchronize()\n",
    "            times.append(time.time() - start)\n",
    "    \n",
    "    avg_time = np.mean(times) * 1000  # Convert to ms\n",
    "    std_time = np.std(times) * 1000\n",
    "    \n",
    "    return {\n",
    "        'game': game_name,\n",
    "        'device': device,\n",
    "        'avg_ms': avg_time,\n",
    "        'std_ms': std_time,\n",
    "        'params': sum(p.numel() for p in model.parameters()) / 1e6,\n",
    "        'max_segments': max_segments,\n",
    "        'optimized': use_optimizations\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Forward pass benchmark function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run forward pass benchmarks\n",
    "forward_results = []\n",
    "\n",
    "games_to_test = ['tictactoe', 'othello']\n",
    "\n",
    "for game in games_to_test:\n",
    "    for device in devices:\n",
    "        try:\n",
    "            result = benchmark_forward_pass(game, device, num_trials=100)\n",
    "            forward_results.append(result)\n",
    "            print(f\"\\n{game} on {device}: {result['avg_ms']:.2f} ¬± {result['std_ms']:.2f} ms\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è  {game} on {device} failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Forward Pass Benchmark Results:\")\n",
    "print(\"=\"*80)\n",
    "for r in forward_results:\n",
    "    print(f\"{r['game']:<12} {r['device']:<6} {r['avg_ms']:>7.2f} ms  ({r['params']:.1f}M params, {r['max_segments']} seg)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Benchmark 2: MCTS Search Speed\n",
    "\n",
    "Measure complete MCTS search including tree traversal + neural evaluations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_mcts_search(game_name, device, num_searches=50, simulations=25):\n",
    "    \"\"\"Benchmark MCTS search speed.\"\"\"\n",
    "    # Load config\n",
    "    with open(f'config/{game_name}.yaml') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    # Override simulations\n",
    "    config['mcts']['simulations'] = simulations\n",
    "    \n",
    "    # Create model\n",
    "    model = HRM(**config['model'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create game class\n",
    "    if game_name == 'tictactoe':\n",
    "        game_class = TicTacToeGame\n",
    "    elif game_name == 'othello':\n",
    "        game_class = OthelloGame\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown game: {game_name}\")\n",
    "    \n",
    "    # Create MCTS\n",
    "    mcts = MCTS(model, game_class, config['mcts'], device=device)\n",
    "    \n",
    "    # Warmup\n",
    "    game = game_class()\n",
    "    for _ in range(5):\n",
    "        _ = mcts.search(game, move_num=0)\n",
    "    \n",
    "    # Benchmark\n",
    "    times = []\n",
    "    for _ in tqdm(range(num_searches), desc=f\"{game_name} MCTS on {device}\"):\n",
    "        game = game_class()\n",
    "        start = time.time()\n",
    "        policy = mcts.search(game, move_num=0)  # Returns policy distribution\n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    std_time = np.std(times)\n",
    "    \n",
    "    return {\n",
    "        'game': game_name,\n",
    "        'device': device,\n",
    "        'simulations': simulations,\n",
    "        'avg_sec': avg_time,\n",
    "        'std_sec': std_time,\n",
    "        'searches_per_sec': 1.0 / avg_time,\n",
    "        'batch_size': config['mcts'].get('batch_size', 1)\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ MCTS benchmark function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run MCTS benchmarks with default config\n",
    "mcts_results = []\n",
    "\n",
    "games_to_test = ['tictactoe', 'othello']\n",
    "\n",
    "for game in games_to_test:\n",
    "    for device in devices:\n",
    "        try:\n",
    "            result = benchmark_mcts_search(game, device, num_searches=50, simulations=25)\n",
    "            mcts_results.append(result)\n",
    "            print(f\"\\n{game} on {device}:\")\n",
    "            print(f\"  {result['avg_sec']:.3f} ¬± {result['std_sec']:.3f} sec/search\")\n",
    "            print(f\"  {result['searches_per_sec']:.1f} searches/sec\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è  {game} on {device} failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MCTS Search Benchmark Results (25 simulations):\")\n",
    "print(\"=\"*80)\n",
    "for r in mcts_results:\n",
    "    print(f\"{r['game']:<12} {r['device']:<6} {r['avg_sec']:>6.3f} s  ({r['searches_per_sec']:>6.1f} searches/s, batch={r['batch_size']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Benchmark 3: Scaling with Simulations\n",
    "\n",
    "How does MCTS performance scale with different numbers of simulations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark MCTS with different simulation counts\n",
    "GAME = \"tictactoe\"  # Change to 'othello' for larger game\n",
    "DEVICE = devices[0]  # Use best available device\n",
    "\n",
    "simulation_counts = [10, 25, 50, 100, 200, 400]\n",
    "scaling_results = []\n",
    "\n",
    "print(f\"Benchmarking {GAME} on {DEVICE} with varying simulations...\\n\")\n",
    "\n",
    "for sims in simulation_counts:\n",
    "    result = benchmark_mcts_search(GAME, DEVICE, num_searches=30, simulations=sims)\n",
    "    scaling_results.append(result)\n",
    "    print(f\"{sims:>4} sims: {result['avg_sec']:.3f} s ({result['searches_per_sec']:.1f} searches/s)\")\n",
    "\n",
    "print(\"\\n‚úÖ Scaling benchmark complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot scaling results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "sims = [r['simulations'] for r in scaling_results]\n",
    "times = [r['avg_sec'] for r in scaling_results]\n",
    "throughput = [r['searches_per_sec'] for r in scaling_results]\n",
    "\n",
    "# Time vs simulations\n",
    "axes[0].plot(sims, times, 'o-', linewidth=2, markersize=8)\n",
    "axes[0].set_xlabel('MCTS Simulations', fontsize=12)\n",
    "axes[0].set_ylabel('Time per Search (seconds)', fontsize=12)\n",
    "axes[0].set_title(f'{GAME.capitalize()} - Search Time vs Simulations', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_xscale('log')\n",
    "\n",
    "# Throughput vs simulations\n",
    "axes[1].plot(sims, throughput, 'o-', linewidth=2, markersize=8, color='green')\n",
    "axes[1].set_xlabel('MCTS Simulations', fontsize=12)\n",
    "axes[1].set_ylabel('Searches per Second', fontsize=12)\n",
    "axes[1].set_title(f'{GAME.capitalize()} - Throughput vs Simulations', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_xscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Benchmark 4: Batching Efficiency\n",
    "\n",
    "Compare batched vs non-batched MCTS evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_batching(game_name, device, batch_sizes=[1, 4, 8, 16, 32]):\n",
    "    \"\"\"Benchmark different batch sizes.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for batch_size in batch_sizes:\n",
    "        # Load config\n",
    "        with open(f'config/{game_name}.yaml') as f:\n",
    "            config = yaml.safe_load(f)\n",
    "        \n",
    "        # Override batch size\n",
    "        config['mcts']['batch_size'] = batch_size\n",
    "        config['mcts']['simulations'] = 100  # Fixed for comparison\n",
    "        \n",
    "        # Create model\n",
    "        model = HRM(**config['model'])\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        \n",
    "        # Create game class\n",
    "        if game_name == 'tictactoe':\n",
    "            game_class = TicTacToeGame\n",
    "        elif game_name == 'othello':\n",
    "            game_class = OthelloGame\n",
    "        \n",
    "        # Create MCTS\n",
    "        mcts = MCTS(model, game_class, config['mcts'], device=device)\n",
    "        \n",
    "        # Warmup\n",
    "        game = game_class()\n",
    "        for _ in range(3):\n",
    "            _ = mcts.search(game, move_num=0)\n",
    "        \n",
    "        # Benchmark\n",
    "        times = []\n",
    "        for _ in range(20):\n",
    "            game = game_class()\n",
    "            start = time.time()\n",
    "            policy = mcts.search(game, move_num=0)  # Returns policy distribution\n",
    "            times.append(time.time() - start)\n",
    "        \n",
    "        avg_time = np.mean(times)\n",
    "        results.append({\n",
    "            'batch_size': batch_size,\n",
    "            'avg_sec': avg_time,\n",
    "            'searches_per_sec': 1.0 / avg_time\n",
    "        })\n",
    "        \n",
    "        print(f\"Batch size {batch_size:>2}: {avg_time:.3f} s ({1.0/avg_time:.1f} searches/s)\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Batching benchmark function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run batching benchmark\n",
    "GAME = \"tictactoe\"\n",
    "DEVICE = devices[0]\n",
    "\n",
    "print(f\"Benchmarking batching for {GAME} on {DEVICE}...\\n\")\n",
    "batching_results = benchmark_batching(GAME, DEVICE, batch_sizes=[1, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048])\n",
    "\n",
    "print(\"\\n‚úÖ Batching benchmark complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot batching efficiency\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "batch_sizes = [r['batch_size'] for r in batching_results]\n",
    "times = [r['avg_sec'] for r in batching_results]\n",
    "throughput = [r['searches_per_sec'] for r in batching_results]\n",
    "\n",
    "# Speedup vs batch size\n",
    "baseline_time = batching_results[0]['avg_sec']  # batch_size=1\n",
    "speedups = [baseline_time / t for t in times]\n",
    "\n",
    "axes[0].plot(batch_sizes, speedups, 'o-', linewidth=2, markersize=8)\n",
    "axes[0].axhline(y=1, color='r', linestyle='--', alpha=0.5, label='No speedup')\n",
    "axes[0].set_xlabel('Batch Size', fontsize=12)\n",
    "axes[0].set_ylabel('Speedup vs Batch=1', fontsize=12)\n",
    "axes[0].set_title(f'{GAME.capitalize()} - Batching Speedup', fontsize=14)\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].legend()\n",
    "axes[0].set_xscale('log', base=2)\n",
    "\n",
    "# Efficiency (speedup / batch_size)\n",
    "efficiency = [speedup / bs for speedup, bs in zip(speedups, batch_sizes)]\n",
    "axes[1].plot(batch_sizes, efficiency, 'o-', linewidth=2, markersize=8, color='orange')\n",
    "axes[1].axhline(y=1, color='r', linestyle='--', alpha=0.5, label='Perfect scaling')\n",
    "axes[1].set_xlabel('Batch Size', fontsize=12)\n",
    "axes[1].set_ylabel('Batching Efficiency', fontsize=12)\n",
    "axes[1].set_title(f'{GAME.capitalize()} - Batching Efficiency', fontsize=14)\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].legend()\n",
    "axes[1].set_xscale('log', base=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Best batch size: {batch_sizes[speedups.index(max(speedups))]} ({max(speedups):.1f}x speedup)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Benchmark 5: End-to-End Game Generation\n",
    "\n",
    "Measure complete game generation time (full playthrough)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_game_generation(game_name, device, num_games=20):\n",
    "    \"\"\"Benchmark full game generation.\"\"\"\n",
    "    # Load config\n",
    "    with open(f'config/{game_name}.yaml') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    config['mcts']['batch_size'] = 2048 \n",
    "    # Create model\n",
    "    model = HRM(**config['model'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Create game class\n",
    "    if game_name == 'tictactoe':\n",
    "        game_class = TicTacToeGame\n",
    "    elif game_name == 'othello':\n",
    "        game_class = OthelloGame\n",
    "    \n",
    "    # Create MCTS\n",
    "    mcts = MCTS(model, game_class, config['mcts'], device=device)\n",
    "    \n",
    "    # Generate games\n",
    "    times = []\n",
    "    move_counts = []\n",
    "    \n",
    "    for _ in tqdm(range(num_games), desc=f\"Generating {game_name} games\"):\n",
    "        game = game_class()\n",
    "        move_num = 0\n",
    "        \n",
    "        start = time.time()\n",
    "        while not game.is_terminal():\n",
    "            policy = mcts.search(game, move_num=move_num)  # Returns policy distribution\n",
    "            # Select move from policy\n",
    "            legal_moves = game.legal_moves()\n",
    "            legal_policy = policy[legal_moves]\n",
    "            move = legal_moves[np.argmax(legal_policy)]\n",
    "            game.make_move(move)\n",
    "            move_num += 1\n",
    "        elapsed = time.time() - start\n",
    "        \n",
    "        times.append(elapsed)\n",
    "        move_counts.append(move_num)\n",
    "    \n",
    "    avg_time = np.mean(times)\n",
    "    avg_moves = np.mean(move_counts)\n",
    "    games_per_hour = 3600 / avg_time\n",
    "    \n",
    "    return {\n",
    "        'game': game_name,\n",
    "        'device': device,\n",
    "        'avg_time': avg_time,\n",
    "        'avg_moves': avg_moves,\n",
    "        'games_per_hour': games_per_hour,\n",
    "        'simulations': config['mcts']['simulations']\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Game generation benchmark function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run game generation benchmarks\n",
    "game_gen_results = []\n",
    "\n",
    "games_to_test = ['tictactoe', 'othello']\n",
    "\n",
    "for game in games_to_test:\n",
    "    for device in devices:\n",
    "        try:\n",
    "            result = benchmark_game_generation(game, device, num_games=20)\n",
    "            game_gen_results.append(result)\n",
    "            print(f\"\\n{game} on {device}:\")\n",
    "            print(f\"  {result['avg_time']:.2f} sec/game ({result['avg_moves']:.1f} moves avg)\")\n",
    "            print(f\"  {result['games_per_hour']:.0f} games/hour\")\n",
    "        except Exception as e:\n",
    "            print(f\"\\n‚ö†Ô∏è  {game} on {device} failed: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Game Generation Benchmark Results:\")\n",
    "print(\"=\"*80)\n",
    "for r in game_gen_results:\n",
    "    print(f\"{r['game']:<12} {r['device']:<6} {r['avg_time']:>6.2f} s/game  ({r['games_per_hour']:>7.0f} games/hr, {r['simulations']} sims)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Benchmark 6: Ray Worker Scaling (Single GPU)\n",
    "\n",
    "Find the optimal number of Ray workers for self-play when sharing a single GPU.\n",
    "\n",
    "**What this measures:**\n",
    "- Self-play throughput (games/hour) vs number of workers\n",
    "- GPU utilization and bottlenecks\n",
    "- CPU vs GPU bound regions\n",
    "- Optimal worker count for maximum throughput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ray worker benchmark function ready\n"
     ]
    }
   ],
   "source": [
    "from training.worker import SelfPlayWorker\n",
    "import games \n",
    "def benchmark_ray_workers(game_name, worker_counts, games_per_worker=5, device='cuda'):\n",
    "    \"\"\"Benchmark self-play throughput vs number of Ray workers.\n",
    "    \n",
    "    Args:\n",
    "        game_name: Game to benchmark ('tictactoe', 'othello')\n",
    "        worker_counts: List of worker counts to test [1, 2, 4, 8, ...]\n",
    "        games_per_worker: Games each worker generates per round\n",
    "        device: Device for GPU inference (all workers share)\n",
    "    \n",
    "    Returns:\n",
    "        List of result dicts with throughput metrics\n",
    "    \"\"\"\n",
    "    # Load config\n",
    "    with open(f'config/{game_name}.yaml') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    results = []\n",
    "    num_gpus = 1\n",
    "    for num_workers in worker_counts:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Testing {num_workers} workers...\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Create workers\n",
    "        workers = []\n",
    "        for i in range(num_workers):\n",
    "            # Assign device to worker\n",
    "            if device == 'cuda':\n",
    "                # Distribute workers across available GPUs\n",
    "                worker_device = f'cuda:{i % num_gpus}'\n",
    "                # Request fractional GPU for each worker (allows multiple workers per GPU)\n",
    "                # For example: 8 workers with 1 GPU ‚Üí each worker gets 0.125 GPU\n",
    "                gpu_fraction = num_gpus / num_workers\n",
    "            elif device == 'mps':\n",
    "                # All workers share the MPS device\n",
    "                worker_device = 'mps'\n",
    "                gpu_fraction = 0\n",
    "            else:\n",
    "                # CPU fallback\n",
    "                worker_device = 'cpu'\n",
    "                gpu_fraction = 0\n",
    "            \n",
    "            # Create worker with appropriate GPU resource allocation\n",
    "            if device == 'cuda' and gpu_fraction > 0:\n",
    "                worker = SelfPlayWorker.options(num_gpus=gpu_fraction).remote(\n",
    "                    worker_id=i,\n",
    "                    game_class=games.TicTacToeGame,\n",
    "                    model_config=config['model'],\n",
    "                    mcts_config=config['mcts'],\n",
    "                    opponent_pool_config={'recent_weight': 0},\n",
    "                    device=worker_device\n",
    "                )\n",
    "            else:\n",
    "                worker = SelfPlayWorker.remote(\n",
    "                    worker_id=i,\n",
    "                    game_class=games.TicTacToeGame,\n",
    "                    model_config=config['model'],\n",
    "                    mcts_config=config['mcts'],\n",
    "                    opponent_pool_config={'recent_weight': 0},\n",
    "                    device=worker_device\n",
    "                )\n",
    "            workers.append(worker)  # FIX: append worker to list\n",
    "\n",
    "        # Warmup (each worker generates 1 game)\n",
    "        warmup_futures = [w.generate_game.remote() for w in workers]\n",
    "        ray.get(warmup_futures)\n",
    "        print(f\"  Warmup complete\")\n",
    "        \n",
    "        # Benchmark: each worker generates multiple games\n",
    "        start_time = time.time()\n",
    "        futures = [worker.generate_batch.remote(games_per_worker) for worker in workers]  # FIX: correct Ray call syntax\n",
    "        \n",
    "        # Wait for all games to complete\n",
    "        # game_results is a list of lists: one flat sample list per worker\n",
    "        worker_results = ray.get(futures)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        # Calculate metrics\n",
    "        # generate_batch returns flat samples (state/policy/value), not per-game dicts\n",
    "        total_games = num_workers * games_per_worker\n",
    "        total_positions = sum(len(samples) for samples in worker_results)\n",
    "        avg_moves = total_positions / total_games if total_games > 0 else 0\n",
    "\n",
    "        games_per_sec = total_games / elapsed if elapsed > 0 else 0\n",
    "        games_per_hour = games_per_sec * 3600\n",
    "        positions_per_sec = total_positions / elapsed if elapsed > 0 else 0\n",
    "        \n",
    "        result = {\n",
    "            'num_workers': num_workers,\n",
    "            'total_games': total_games,\n",
    "            'elapsed_sec': elapsed,\n",
    "            'games_per_sec': games_per_sec,\n",
    "            'games_per_hour': games_per_hour,\n",
    "            'positions_per_sec': positions_per_sec,\n",
    "            'avg_moves': avg_moves\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        print(f\"  Total games: {total_games}\")\n",
    "        print(f\"  Time: {elapsed:.1f}s\")\n",
    "        print(f\"  Throughput: {games_per_sec:.2f} games/s ({games_per_hour:.0f} games/hr)\")\n",
    "        print(f\"  Positions/sec: {positions_per_sec:.1f}\")\n",
    "        \n",
    "        # Cleanup workers\n",
    "        for worker in workers:\n",
    "            ray.kill(worker)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"‚úÖ Ray worker benchmark function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-25 22:33:30,261\tINFO worker.py:2013 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ray initialized\n",
      "   CPU cores: 2\n",
      "   Available memory: 10.7 GB\n",
      "Benchmarking Ray workers for tictactoe on cuda\n",
      "Worker counts to test: [1, 2, 4, 8, 16, 32]\n",
      "Each worker will generate 5 games per test\n",
      "\n",
      "\n",
      "============================================================\n",
      "Testing 1 workers...\n",
      "============================================================\n",
      "\u001b[36m(SelfPlayWorker pid=13139)\u001b[0m ‚úÖ FlashAttention (SDPA) enabled\n",
      "  Warmup complete\n",
      "  Total games: 5\n",
      "  Time: 1.8s\n",
      "  Throughput: 2.74 games/s (9865 games/hr)\n",
      "  Positions/sec: 12.1\n",
      "\n",
      "============================================================\n",
      "Testing 2 workers...\n",
      "============================================================\n",
      "\u001b[36m(SelfPlayWorker pid=13140)\u001b[0m ‚úÖ FlashAttention (SDPA) enabled\n",
      "\u001b[36m(SelfPlayWorker pid=13240)\u001b[0m ‚úÖ FlashAttention (SDPA) enabled\n",
      "  Warmup complete\n",
      "  Total games: 10\n",
      "  Time: 2.7s\n",
      "  Throughput: 3.65 games/s (13155 games/hr)\n",
      "  Positions/sec: 15.7\n",
      "\n",
      "============================================================\n",
      "Testing 4 workers...\n",
      "============================================================\n",
      "\u001b[36m(SelfPlayWorker pid=13322)\u001b[0m ‚úÖ FlashAttention (SDPA) enabled\n",
      "\u001b[36m(SelfPlayWorker pid=13323)\u001b[0m ‚úÖ FlashAttention (SDPA) enabled\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-984/1583391457.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     ray_worker_results = benchmark_ray_workers(\n\u001b[0m\u001b[1;32m     37\u001b[0m         \u001b[0mgame_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mGAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mworker_counts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworker_counts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipython-input-984/1070833995.py\u001b[0m in \u001b[0;36mbenchmark_ray_workers\u001b[0;34m(game_name, worker_counts, games_per_worker, device)\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0;31m# Warmup (each worker generates 1 game)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mwarmup_futures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_game\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremote\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mray\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwarmup_futures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"  Warmup complete\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\u001b[0m in \u001b[0;36mauto_init_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mauto_init_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mauto_init_ray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mauto_init_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"init\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_client_mode_enabled_by_default\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(object_refs, timeout, _use_object_store)\u001b[0m\n\u001b[1;32m   2979\u001b[0m             )\n\u001b[1;32m   2980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2981\u001b[0;31m         values, debugger_breakpoint = worker.get_objects(\n\u001b[0m\u001b[1;32m   2982\u001b[0m             \u001b[0mobject_refs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_object_store\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_use_object_store\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2983\u001b[0m         )\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\u001b[0m in \u001b[0;36mget_objects\u001b[0;34m(self, object_refs, timeout, return_exceptions, skip_deserialization, use_object_store)\u001b[0m\n\u001b[1;32m    980\u001b[0m         serialized_objects: List[\n\u001b[1;32m    981\u001b[0m             \u001b[0mserialization\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSerializedRayObject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 982\u001b[0;31m         \u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore_worker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    983\u001b[0m             \u001b[0mobject_refs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m             \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpython/ray/_raylet.pyx\u001b[0m in \u001b[0;36mray._raylet.CoreWorker.get_objects\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpython/ray/includes/common.pxi\u001b[0m in \u001b[0;36mray._raylet.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run Ray worker scaling benchmark\n",
    "\n",
    "import ray\n",
    "import psutil\n",
    "import multiprocessing as mp\n",
    "\n",
    "# Check Ray availability\n",
    "for _ in range(2):\n",
    "    try:\n",
    "        if not ray.is_initialized():\n",
    "            ray.init(ignore_reinit_error=True)\n",
    "        print(f\"‚úÖ Ray initialized\")\n",
    "        print(f\"   CPU cores: {psutil.cpu_count()}\")\n",
    "        print(f\"   Available memory: {psutil.virtual_memory().available / 1e9:.1f} GB\")\n",
    "        break\n",
    "    except Exception as e:\n",
    "        %pip install ray\n",
    "        print(f\"‚ö†Ô∏è  Ray not available: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "GAME = \"tictactoe\"  # Use tictactoe for faster iteration\n",
    "DEVICE = 'cuda' if 'cuda' in devices else 'cpu'\n",
    "\n",
    "# Test worker counts: 1, 2, 4, 8, 16, 32\n",
    "# Adjust based on available CPU cores\n",
    "# max_workers = min(32, psutil.cpu_count())\n",
    "worker_counts = [1, 2, 4, 8, 16, 32]\n",
    "# worker_counts = [w for w in worker_counts if w <= max_workers]\n",
    "\n",
    "print(f\"Benchmarking Ray workers for {GAME} on {DEVICE}\")\n",
    "print(f\"Worker counts to test: {worker_counts}\")\n",
    "print(f\"Each worker will generate 5 games per test\\n\")\n",
    "\n",
    "try:\n",
    "    ray_worker_results = benchmark_ray_workers(\n",
    "        game_name=GAME,\n",
    "        worker_counts=worker_counts,\n",
    "        games_per_worker=5,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"RAY WORKER SCALING RESULTS\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"{'Workers':<10} {'Games/sec':<12} {'Games/hour':<15} {'Positions/sec':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    for r in ray_worker_results:\n",
    "        print(f\"{r['num_workers']:<10} {r['games_per_sec']:<12.2f} {r['games_per_hour']:<15.0f} {r['positions_per_sec']:<15.1f}\")\n",
    "    \n",
    "    print(\"\\n‚úÖ Ray worker benchmark complete\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Ray worker benchmark failed: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    ray_worker_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Ray worker scaling results\n",
    "if ray_worker_results:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "    \n",
    "    workers = [r['num_workers'] for r in ray_worker_results]\n",
    "    throughput = [r['games_per_hour'] for r in ray_worker_results]\n",
    "    positions_per_sec = [r['positions_per_sec'] for r in ray_worker_results]\n",
    "    \n",
    "    # Calculate speedup vs single worker\n",
    "    baseline_throughput = ray_worker_results[0]['games_per_hour']\n",
    "    speedups = [t / baseline_throughput for t in throughput]\n",
    "    efficiency = [speedup / w for speedup, w in zip(speedups, workers)]\n",
    "    \n",
    "    # Plot 1: Throughput vs workers\n",
    "    axes[0].plot(workers, throughput, 'o-', linewidth=2, markersize=8, color='blue')\n",
    "    axes[0].set_xlabel('Number of Workers', fontsize=12)\n",
    "    axes[0].set_ylabel('Games per Hour', fontsize=12)\n",
    "    axes[0].set_title(f'{GAME.capitalize()} - Self-Play Throughput', fontsize=14)\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    axes[0].set_xscale('log', base=2)\n",
    "    \n",
    "    # Plot 2: Speedup vs workers\n",
    "    axes[1].plot(workers, speedups, 'o-', linewidth=2, markersize=8, color='green')\n",
    "    axes[1].plot(workers, workers, '--', alpha=0.5, color='red', label='Linear scaling')\n",
    "    axes[1].set_xlabel('Number of Workers', fontsize=12)\n",
    "    axes[1].set_ylabel('Speedup vs 1 Worker', fontsize=12)\n",
    "    axes[1].set_title(f'{GAME.capitalize()} - Scaling Efficiency', fontsize=14)\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    axes[1].legend()\n",
    "    axes[1].set_xscale('log', base=2)\n",
    "    axes[1].set_yscale('log', base=2)\n",
    "    \n",
    "    # Plot 3: Parallel efficiency\n",
    "    axes[2].plot(workers, efficiency, 'o-', linewidth=2, markersize=8, color='orange')\n",
    "    axes[2].axhline(y=1, color='r', linestyle='--', alpha=0.5, label='Perfect efficiency')\n",
    "    axes[2].axhline(y=0.8, color='orange', linestyle='--', alpha=0.5, label='80% efficiency')\n",
    "    axes[2].set_xlabel('Number of Workers', fontsize=12)\n",
    "    axes[2].set_ylabel('Parallel Efficiency', fontsize=12)\n",
    "    axes[2].set_title(f'{GAME.capitalize()} - Worker Efficiency', fontsize=14)\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    axes[2].legend()\n",
    "    axes[2].set_xscale('log', base=2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find optimal worker count (best efficiency above 80% or max throughput)\n",
    "    optimal_idx = -1\n",
    "    for i, (eff, w) in enumerate(zip(efficiency, workers)):\n",
    "        if eff >= 0.8:\n",
    "            optimal_idx = i\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    if optimal_idx >= 0:\n",
    "        optimal_workers = workers[optimal_idx]\n",
    "        optimal_throughput = throughput[optimal_idx]\n",
    "        optimal_speedup = speedups[optimal_idx]\n",
    "        \n",
    "        print(f\"\\nüìä Optimal Configuration:\")\n",
    "        print(f\"  Workers: {optimal_workers}\")\n",
    "        print(f\"  Throughput: {optimal_throughput:.0f} games/hour\")\n",
    "        print(f\"  Speedup: {optimal_speedup:.1f}x\")\n",
    "        print(f\"  Efficiency: {efficiency[optimal_idx]*100:.1f}%\")\n",
    "    else:\n",
    "        # Just use max throughput\n",
    "        max_idx = throughput.index(max(throughput))\n",
    "        print(f\"\\nüìä Best Performance:\")\n",
    "        print(f\"  Workers: {workers[max_idx]}\")\n",
    "        print(f\"  Throughput: {throughput[max_idx]:.0f} games/hour\")\n",
    "        print(f\"  Speedup: {speedups[max_idx]:.1f}x\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No Ray worker results to plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Export Results\n",
    "\n",
    "Save benchmark results for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to file\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "results = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'devices': devices,\n",
    "    'forward_pass': forward_results,\n",
    "    'mcts_search': mcts_results,\n",
    "    'game_generation': game_gen_results,\n",
    "    'scaling': scaling_results if 'scaling_results' in dir() else [],\n",
    "    'batching': batching_results if 'batching_results' in dir() else [],\n",
    "    'ray_workers': ray_worker_results if 'ray_worker_results' in dir() and ray_worker_results else []\n",
    "}\n",
    "\n",
    "output_file = f\"benchmark_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Results saved to: {output_file}\")\n",
    "print(f\"\\nFile size: {Path(output_file).stat().st_size / 1024:.1f} KB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
