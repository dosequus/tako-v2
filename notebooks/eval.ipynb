{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tako HRM - Evaluation\n",
    "\n",
    "Evaluate trained models against baseline opponents.\n",
    "\n",
    "## Evaluation Methods\n",
    "\n",
    "- **Random baseline** - Win rate vs random play\n",
    "- **Self-play** - Model vs older checkpoints\n",
    "- **External engines** - Stockfish (chess), Edax (othello), etc.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Setup\n",
    "\n",
    "**Run `setup.ipynb` first if you haven't already!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /content\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['.config', 'sample_data']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure we're in the repo directory\n",
    "import os\n",
    "# if not os.path.exists('scripts/eval.py'):\n",
    "#     os.chdir('tako-v2')\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## TicTacToe Evaluation\n",
    "\n",
    "Test model against random play and perfect play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU: NVIDIA L4\n",
      "✅ Setup verified - ready to evaluate!\n"
     ]
    }
   ],
   "source": [
    "# Verify setup has been run\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.chdir('/content/tako-v2')\n",
    "if not os.path.exists('scripts/eval.py'):\n",
    "    print(\"❌ ERROR: Not in tako-v2 directory\")\n",
    "    print(\"   Run setup.ipynb first!\")\n",
    "    raise FileNotFoundError(\"Run setup.ipynb first\")\n",
    "\n",
    "sys.path.insert(0, os.getcwd())\n",
    "\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(f\"✅ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "    print(f\"✅ MPS: Apple Silicon GPU\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(f\"ℹ️  CPU only\")\n",
    "\n",
    "print(f\"✅ Setup verified - ready to evaluate!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint: checkpoint_5000.pt\n",
      "Path: checkpoints/tictactoe/checkpoint_5000.pt\n"
     ]
    }
   ],
   "source": [
    "# Find latest Othello checkpoint\n",
    "from pathlib import Path\n",
    "\n",
    "ckpt_dir = Path('checkpoints/tictactoe')\n",
    "if ckpt_dir.exists():\n",
    "    checkpoints = sorted(ckpt_dir.glob('*.pt'), key=lambda p: p.stat().st_mtime)\n",
    "    if checkpoints:\n",
    "        latest_ckpt = checkpoints[-1]\n",
    "        print(f\"Latest checkpoint: {latest_ckpt.name}\")\n",
    "        print(f\"Path: {latest_ckpt}\")\n",
    "    else:\n",
    "        print(\"⚠️  No checkpoints found. Train first.\")\n",
    "        latest_ckpt = None\n",
    "else:\n",
    "    print(\"⚠️  Checkpoint directory not found\")\n",
    "    latest_ckpt = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating TicTacToe model vs random play...\n",
      "\n",
      "================================================================================\n",
      "[Eval] Evaluating checkpoint: checkpoints/tictactoe/checkpoint_5000.pt\n",
      "[Eval] Opponent: random\n",
      "[Eval] Games: 100\n",
      "[Eval] Device: cuda\n",
      "[Eval] Game: tictactoe\n",
      "[Eval] MCTS simulations: 400\n",
      "[Eval] Playing 100 games vs random player...\n",
      "✅ FlashAttention (SDPA) enabled\n",
      "vs Random: 100% 100/100 [01:49<00:00,  1.09s/game, W=79, L=7, D=14, WR=0.790]\n",
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS\n",
      "==================================================\n",
      "Total games:  100\n",
      "Wins:         79\n",
      "Losses:       7\n",
      "Draws:        14\n",
      "Win rate:     0.790\n",
      "==================================================\n",
      "\n",
      "Good: Model is learning.\n",
      "\n",
      "================================================================================\n",
      "Expected: >90% win rate after convergence\n"
     ]
    }
   ],
   "source": [
    "# Evaluate vs random opponent\n",
    "if latest_ckpt:\n",
    "    print(\"Evaluating TicTacToe model vs random play...\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    !/usr/local/bin/uv run python scripts/eval.py \\\n",
    "        --config config/tictactoe.yaml \\\n",
    "        --checkpoint {latest_ckpt} \\\n",
    "        --opponent random \\\n",
    "        --games 100 \\\n",
    "        --device {device}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Expected: >90% win rate after convergence\")\n",
    "else:\n",
    "    print(\"❌ No checkpoint available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating TicTacToe model vs perfect play...\n",
      "\n",
      "================================================================================\n",
      "[Eval] Evaluating checkpoint: checkpoints/tictactoe/checkpoint_20000.pt\n",
      "[Eval] Opponent: minimax\n",
      "[Eval] Games: 50\n",
      "[Eval] Device: cuda\n",
      "[Eval] Game: tictactoe\n",
      "[Eval] MCTS simulations: 400\n",
      "[Eval] Playing 50 games vs minimax (depth=-1)...\n",
      "✅ FlashAttention (SDPA) enabled\n",
      "vs Minimax: 100% 50/50 [01:23<00:00,  1.67s/game, W=0, L=3, D=47, WR=0.000]\n",
      "\n",
      "==================================================\n",
      "EVALUATION RESULTS\n",
      "==================================================\n",
      "Total games:  50\n",
      "Wins:         0\n",
      "Losses:       3\n",
      "Draws:        47\n",
      "Win rate:     0.000\n",
      "==================================================\n",
      "\n",
      "Near-converged: 94.0% non-loss rate vs minimax.\n",
      "\n",
      "================================================================================\n",
      "Expected: 0% losses (draws or wins only)\n"
     ]
    }
   ],
   "source": [
    "# Evaluate vs perfect minimax opponent (TicTacToe only)\n",
    "if latest_ckpt:\n",
    "    print(\"Evaluating TicTacToe model vs perfect play...\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    !/usr/local/bin/uv run python scripts/eval.py \\\n",
    "        --config config/tictactoe.yaml \\\n",
    "        --checkpoint {latest_ckpt} \\\n",
    "        --opponent minimax \\\n",
    "        --games 50 \\\n",
    "        --device {device}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Expected: 0% losses (draws or wins only)\")\n",
    "else:\n",
    "    print(\"❌ No checkpoint available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Othello Evaluation\n",
    "\n",
    "Test model against random play and Edax engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find latest Othello checkpoint\n",
    "from pathlib import Path\n",
    "\n",
    "ckpt_dir = Path('checkpoints/othello')\n",
    "if ckpt_dir.exists():\n",
    "    checkpoints = sorted(ckpt_dir.glob('*.pt'), key=lambda p: p.stat().st_mtime)\n",
    "    if checkpoints:\n",
    "        latest_ckpt = checkpoints[-1]\n",
    "        print(f\"Latest checkpoint: {latest_ckpt.name}\")\n",
    "        print(f\"Path: {latest_ckpt}\")\n",
    "    else:\n",
    "        print(\"⚠️  No checkpoints found. Train first.\")\n",
    "        latest_ckpt = None\n",
    "else:\n",
    "    print(\"⚠️  Checkpoint directory not found\")\n",
    "    latest_ckpt = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate vs random opponent\n",
    "if latest_ckpt:\n",
    "    print(\"Evaluating Othello model vs random play...\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    !/usr/local/bin/uv run python scripts/eval.py \\\n",
    "        --config config/othello.yaml \\\n",
    "        --checkpoint {latest_ckpt} \\\n",
    "        --opponent random \\\n",
    "        --games 100 \\\n",
    "        --device {device}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Expected: >95% win rate after training\")\n",
    "else:\n",
    "    print(\"❌ No checkpoint available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate vs Edax engine (if available)\n",
    "if latest_ckpt:\n",
    "    print(\"Evaluating Othello model vs Edax level 3...\")\n",
    "    print(\"Note: Requires Edax installed\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    !/usr/local/bin/uv run python scripts/eval.py \\\n",
    "        --config config/othello.yaml \\\n",
    "        --checkpoint {latest_ckpt} \\\n",
    "        --opponent edax \\\n",
    "        --opponent-level 3 \\\n",
    "        --games 50 \\\n",
    "        --device {device}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Target: Beat Edax level 3 (Phase 1 goal)\")\n",
    "else:\n",
    "    print(\"❌ No checkpoint available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hex Evaluation\n",
    "\n",
    "Test model against random play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find latest Hex checkpoint\n",
    "from pathlib import Path\n",
    "\n",
    "ckpt_dir = Path('checkpoints/hex')\n",
    "if ckpt_dir.exists():\n",
    "    checkpoints = sorted(ckpt_dir.glob('*.pt'), key=lambda p: p.stat().st_mtime)\n",
    "    if checkpoints:\n",
    "        latest_ckpt = checkpoints[-1]\n",
    "        print(f\"Latest checkpoint: {latest_ckpt.name}\")\n",
    "        print(f\"Path: {latest_ckpt}\")\n",
    "    else:\n",
    "        print(\"⚠️  No checkpoints found. Train first.\")\n",
    "        latest_ckpt = None\n",
    "else:\n",
    "    print(\"⚠️  Checkpoint directory not found\")\n",
    "    latest_ckpt = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate vs random opponent\n",
    "if latest_ckpt:\n",
    "    print(\"Evaluating Hex model vs random play...\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    !/usr/local/bin/uv run python scripts/eval.py \\\n",
    "        --config config/hex.yaml \\\n",
    "        --checkpoint {latest_ckpt} \\\n",
    "        --opponent random \\\n",
    "        --games 100 \\\n",
    "        --device {device}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Expected: >90% win rate after training\")\n",
    "else:\n",
    "    print(\"❌ No checkpoint available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Chess Evaluation\n",
    "\n",
    "Test model against Stockfish at various levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find latest Chess checkpoint\n",
    "from pathlib import Path\n",
    "\n",
    "ckpt_dir = Path('checkpoints/chess')\n",
    "if ckpt_dir.exists():\n",
    "    checkpoints = sorted(ckpt_dir.glob('*.pt'), key=lambda p: p.stat().st_mtime)\n",
    "    if checkpoints:\n",
    "        latest_ckpt = checkpoints[-1]\n",
    "        print(f\"Latest checkpoint: {latest_ckpt.name}\")\n",
    "        print(f\"Path: {latest_ckpt}\")\n",
    "    else:\n",
    "        print(\"⚠️  No checkpoints found. Train first.\")\n",
    "        latest_ckpt = None\n",
    "else:\n",
    "    print(\"⚠️  Checkpoint directory not found\")\n",
    "    latest_ckpt = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate vs Stockfish level 5\n",
    "if latest_ckpt:\n",
    "    print(\"Evaluating Chess model vs Stockfish level 5...\")\n",
    "    print(\"Note: Requires Stockfish installed\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    !/usr/local/bin/uv run python scripts/eval.py \\\n",
    "        --config config/chess.yaml \\\n",
    "        --checkpoint {latest_ckpt} \\\n",
    "        --opponent stockfish \\\n",
    "        --opponent-level 5 \\\n",
    "        --games 50 \\\n",
    "        --device {device}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Phase 3 target: ~1700 Elo (after pretraining)\")\n",
    "    print(\"Phase 5 target: 2500+ Elo (GM level)\")\n",
    "else:\n",
    "    print(\"❌ No checkpoint available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Compare Multiple Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare progression across checkpoints\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "GAME = \"tictactoe\"  # Change to othello, hex, chess\n",
    "\n",
    "ckpt_dir = Path(f'checkpoints/{GAME}')\n",
    "if ckpt_dir.exists():\n",
    "    checkpoints = sorted(ckpt_dir.glob('step_*.pt'), key=lambda p: int(re.search(r'step_(\\d+)', p.name).group(1)))\n",
    "    \n",
    "    if len(checkpoints) > 5:\n",
    "        # Sample 5 checkpoints evenly\n",
    "        indices = [0, len(checkpoints)//4, len(checkpoints)//2, 3*len(checkpoints)//4, -1]\n",
    "        sample_ckpts = [checkpoints[i] for i in indices]\n",
    "        \n",
    "        print(f\"Comparing {len(sample_ckpts)} {GAME} checkpoints:\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        steps = []\n",
    "        win_rates = []\n",
    "        \n",
    "        for ckpt in sample_ckpts:\n",
    "            step = int(re.search(r'step_(\\d+)', ckpt.name).group(1))\n",
    "            steps.append(step)\n",
    "            \n",
    "            print(f\"\\nEvaluating checkpoint: {ckpt.name}\")\n",
    "            # Run eval and parse output\n",
    "            # This is a placeholder - actual implementation would parse eval output\n",
    "            print(f\"  (Eval not implemented in comparison mode yet)\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Use individual eval cells above for detailed results\")\n",
    "    else:\n",
    "        print(f\"Found {len(checkpoints)} checkpoints - need at least 5 for comparison\")\n",
    "else:\n",
    "    print(f\"⚠️  No checkpoints found for {GAME}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
