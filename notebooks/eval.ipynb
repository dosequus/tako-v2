{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tako HRM - Evaluation\n",
    "\n",
    "Evaluate trained models against baseline opponents.\n",
    "\n",
    "## Evaluation Methods\n",
    "\n",
    "- **Random baseline** - Win rate vs random play\n",
    "- **Self-play** - Model vs older checkpoints\n",
    "- **External engines** - Stockfish (chess), Edax (othello), etc.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup (Run Once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we're in the repo directory\n",
    "import os\n",
    "if not os.path.exists('scripts/eval.py'):\n",
    "    os.chdir('tako-v2')\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print(f\"✅ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"ℹ️  Using CPU for evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## TicTacToe Evaluation\n",
    "\n",
    "Test model against random play and perfect play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find latest TicTacToe checkpoint\n",
    "from pathlib import Path\n",
    "\n",
    "ckpt_dir = Path('checkpoints/tictactoe')\n",
    "if ckpt_dir.exists():\n",
    "    checkpoints = sorted(ckpt_dir.glob('*.pt'), key=lambda p: p.stat().st_mtime)\n",
    "    if checkpoints:\n",
    "        latest_ckpt = checkpoints[-1]\n",
    "        print(f\"Latest checkpoint: {latest_ckpt.name}\")\n",
    "        print(f\"Path: {latest_ckpt}\")\n",
    "    else:\n",
    "        print(\"⚠️  No checkpoints found. Train first.\")\n",
    "        latest_ckpt = None\n",
    "else:\n",
    "    print(\"⚠️  Checkpoint directory not found\")\n",
    "    latest_ckpt = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate vs random opponent\n",
    "if latest_ckpt:\n",
    "    print(\"Evaluating TicTacToe model vs random play...\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    !~/.cargo/bin/uv run python scripts/eval.py \\\n",
    "        --config config/tictactoe.yaml \\\n",
    "        --checkpoint {latest_ckpt} \\\n",
    "        --opponent random \\\n",
    "        --games 100 \\\n",
    "        --device {device}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Expected: >90% win rate after convergence\")\n",
    "else:\n",
    "    print(\"❌ No checkpoint available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate vs perfect minimax opponent (TicTacToe only)\n",
    "if latest_ckpt:\n",
    "    print(\"Evaluating TicTacToe model vs perfect play...\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    !~/.cargo/bin/uv run python scripts/eval.py \\\n",
    "        --config config/tictactoe.yaml \\\n",
    "        --checkpoint {latest_ckpt} \\\n",
    "        --opponent minimax \\\n",
    "        --games 50 \\\n",
    "        --device {device}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Expected: 0% losses (draws or wins only)\")\n",
    "else:\n",
    "    print(\"❌ No checkpoint available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Othello Evaluation\n",
    "\n",
    "Test model against random play and Edax engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find latest Othello checkpoint\n",
    "from pathlib import Path\n",
    "\n",
    "ckpt_dir = Path('checkpoints/othello')\n",
    "if ckpt_dir.exists():\n",
    "    checkpoints = sorted(ckpt_dir.glob('*.pt'), key=lambda p: p.stat().st_mtime)\n",
    "    if checkpoints:\n",
    "        latest_ckpt = checkpoints[-1]\n",
    "        print(f\"Latest checkpoint: {latest_ckpt.name}\")\n",
    "        print(f\"Path: {latest_ckpt}\")\n",
    "    else:\n",
    "        print(\"⚠️  No checkpoints found. Train first.\")\n",
    "        latest_ckpt = None\n",
    "else:\n",
    "    print(\"⚠️  Checkpoint directory not found\")\n",
    "    latest_ckpt = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate vs random opponent\n",
    "if latest_ckpt:\n",
    "    print(\"Evaluating Othello model vs random play...\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    !~/.cargo/bin/uv run python scripts/eval.py \\\n",
    "        --config config/othello.yaml \\\n",
    "        --checkpoint {latest_ckpt} \\\n",
    "        --opponent random \\\n",
    "        --games 100 \\\n",
    "        --device {device}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Expected: >95% win rate after training\")\n",
    "else:\n",
    "    print(\"❌ No checkpoint available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate vs Edax engine (if available)\n",
    "if latest_ckpt:\n",
    "    print(\"Evaluating Othello model vs Edax level 3...\")\n",
    "    print(\"Note: Requires Edax installed\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    !~/.cargo/bin/uv run python scripts/eval.py \\\n",
    "        --config config/othello.yaml \\\n",
    "        --checkpoint {latest_ckpt} \\\n",
    "        --opponent edax \\\n",
    "        --opponent-level 3 \\\n",
    "        --games 50 \\\n",
    "        --device {device}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Target: Beat Edax level 3 (Phase 1 goal)\")\n",
    "else:\n",
    "    print(\"❌ No checkpoint available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Hex Evaluation\n",
    "\n",
    "Test model against random play."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find latest Hex checkpoint\n",
    "from pathlib import Path\n",
    "\n",
    "ckpt_dir = Path('checkpoints/hex')\n",
    "if ckpt_dir.exists():\n",
    "    checkpoints = sorted(ckpt_dir.glob('*.pt'), key=lambda p: p.stat().st_mtime)\n",
    "    if checkpoints:\n",
    "        latest_ckpt = checkpoints[-1]\n",
    "        print(f\"Latest checkpoint: {latest_ckpt.name}\")\n",
    "        print(f\"Path: {latest_ckpt}\")\n",
    "    else:\n",
    "        print(\"⚠️  No checkpoints found. Train first.\")\n",
    "        latest_ckpt = None\n",
    "else:\n",
    "    print(\"⚠️  Checkpoint directory not found\")\n",
    "    latest_ckpt = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate vs random opponent\n",
    "if latest_ckpt:\n",
    "    print(\"Evaluating Hex model vs random play...\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    !~/.cargo/bin/uv run python scripts/eval.py \\\n",
    "        --config config/hex.yaml \\\n",
    "        --checkpoint {latest_ckpt} \\\n",
    "        --opponent random \\\n",
    "        --games 100 \\\n",
    "        --device {device}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Expected: >90% win rate after training\")\n",
    "else:\n",
    "    print(\"❌ No checkpoint available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Chess Evaluation\n",
    "\n",
    "Test model against Stockfish at various levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find latest Chess checkpoint\n",
    "from pathlib import Path\n",
    "\n",
    "ckpt_dir = Path('checkpoints/chess')\n",
    "if ckpt_dir.exists():\n",
    "    checkpoints = sorted(ckpt_dir.glob('*.pt'), key=lambda p: p.stat().st_mtime)\n",
    "    if checkpoints:\n",
    "        latest_ckpt = checkpoints[-1]\n",
    "        print(f\"Latest checkpoint: {latest_ckpt.name}\")\n",
    "        print(f\"Path: {latest_ckpt}\")\n",
    "    else:\n",
    "        print(\"⚠️  No checkpoints found. Train first.\")\n",
    "        latest_ckpt = None\n",
    "else:\n",
    "    print(\"⚠️  Checkpoint directory not found\")\n",
    "    latest_ckpt = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate vs Stockfish level 5\n",
    "if latest_ckpt:\n",
    "    print(\"Evaluating Chess model vs Stockfish level 5...\")\n",
    "    print(\"Note: Requires Stockfish installed\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    !~/.cargo/bin/uv run python scripts/eval.py \\\n",
    "        --config config/chess.yaml \\\n",
    "        --checkpoint {latest_ckpt} \\\n",
    "        --opponent stockfish \\\n",
    "        --opponent-level 5 \\\n",
    "        --games 50 \\\n",
    "        --device {device}\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"Phase 3 target: ~1700 Elo (after pretraining)\")\n",
    "    print(\"Phase 5 target: 2500+ Elo (GM level)\")\n",
    "else:\n",
    "    print(\"❌ No checkpoint available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Compare Multiple Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare progression across checkpoints\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "GAME = \"tictactoe\"  # Change to othello, hex, chess\n",
    "\n",
    "ckpt_dir = Path(f'checkpoints/{GAME}')\n",
    "if ckpt_dir.exists():\n",
    "    checkpoints = sorted(ckpt_dir.glob('step_*.pt'), key=lambda p: int(re.search(r'step_(\\d+)', p.name).group(1)))\n",
    "    \n",
    "    if len(checkpoints) > 5:\n",
    "        # Sample 5 checkpoints evenly\n",
    "        indices = [0, len(checkpoints)//4, len(checkpoints)//2, 3*len(checkpoints)//4, -1]\n",
    "        sample_ckpts = [checkpoints[i] for i in indices]\n",
    "        \n",
    "        print(f\"Comparing {len(sample_ckpts)} {GAME} checkpoints:\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        steps = []\n",
    "        win_rates = []\n",
    "        \n",
    "        for ckpt in sample_ckpts:\n",
    "            step = int(re.search(r'step_(\\d+)', ckpt.name).group(1))\n",
    "            steps.append(step)\n",
    "            \n",
    "            print(f\"\\nEvaluating checkpoint: {ckpt.name}\")\n",
    "            # Run eval and parse output\n",
    "            # This is a placeholder - actual implementation would parse eval output\n",
    "            print(f\"  (Eval not implemented in comparison mode yet)\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"Use individual eval cells above for detailed results\")\n",
    "    else:\n",
    "        print(f\"Found {len(checkpoints)} checkpoints - need at least 5 for comparison\")\n",
    "else:\n",
    "    print(f\"⚠️  No checkpoints found for {GAME}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
